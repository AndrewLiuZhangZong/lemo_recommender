"""
Flink Job 2: ç‰©å“çƒ­åº¦å®æ—¶è®¡ç®—
åŠŸèƒ½ï¼šä»Kafkaæ¶ˆè´¹ç”¨æˆ·è¡Œä¸ºæ•°æ®ï¼Œå®æ—¶è®¡ç®—ç‰©å“çƒ­åº¦ï¼Œå†™å…¥Redis
"""
import json
import sys
import math
from datetime import datetime, timedelta
from collections import defaultdict

try:
    from pyflink.datastream import StreamExecutionEnvironment
    from pyflink.datastream.window import SlidingProcessingTimeWindows
    from pyflink.common import Time, WatermarkStrategy
    from pyflink.datastream.functions import ProcessWindowFunction
    from pyflink.datastream.connectors.kafka import KafkaSource, KafkaOffsetsInitializer
    from pyflink.common.serialization import SimpleStringSchema
    PYFLINK_AVAILABLE = True
except ImportError:
    PYFLINK_AVAILABLE = False
    print("âš ï¸  PyFlinkæœªå®‰è£…ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼")


class HotScoreCalculator(ProcessWindowFunction):
    """ç‰©å“çƒ­åº¦è®¡ç®—å‡½æ•°"""
    
    def __init__(self, action_weights, decay_lambda):
        self.action_weights = action_weights
        self.decay_lambda = decay_lambda
    
    def process(self, key, context, elements):
        """
        è®¡ç®—çª—å£å†…ç‰©å“çš„çƒ­åº¦åˆ†æ•°
        
        Args:
            key: (tenant_id, scenario_id, item_id)
            context: çª—å£ä¸Šä¸‹æ–‡
            elements: çª—å£å†…çš„æ‰€æœ‰è¡Œä¸º
        """
        tenant_id, scenario_id, item_id = key
        
        # ç»Ÿè®¡å„ç±»è¡Œä¸ºæ•°é‡
        action_counts = defaultdict(int)
        total_duration = 0
        user_set = set()
        
        for behavior in elements:
            action_type = behavior.get('action_type')
            action_counts[action_type] += 1
            
            # ç»Ÿè®¡ç”¨æˆ·æ•°ï¼ˆå»é‡ï¼‰
            user_set.add(behavior.get('user_id'))
            
            # ç»Ÿè®¡è§‚çœ‹æ—¶é•¿
            extra = behavior.get('extra', {})
            if 'duration' in extra:
                try:
                    total_duration += float(extra['duration'])
                except:
                    pass
        
        # è®¡ç®—åŸºç¡€çƒ­åº¦åˆ†æ•°
        base_score = 0
        for action_type, count in action_counts.items():
            weight = self.action_weights.get(action_type, 1.0)
            base_score += count * weight
        
        # æ—¶é—´è¡°å‡ï¼ˆåŸºäºçª—å£ç»“æŸæ—¶é—´ï¼‰
        now = datetime.now()
        window_end = datetime.fromtimestamp(context.window().end / 1000)
        days_ago = (now - window_end).days
        decay_factor = math.exp(-self.decay_lambda * days_ago)
        
        # æœ€ç»ˆçƒ­åº¦åˆ†æ•°
        hot_score = base_score * decay_factor
        
        # æ„é€ ç»“æœ
        result = {
            'tenant_id': tenant_id,
            'scenario_id': scenario_id,
            'item_id': item_id,
            'hot_score': round(hot_score, 2),
            'stats': {
                'view_count': action_counts.get('view', 0),
                'click_count': action_counts.get('click', 0),
                'like_count': action_counts.get('like', 0),
                'share_count': action_counts.get('share', 0),
                'unique_users': len(user_set),
                'avg_duration': round(total_duration / len(list(elements)), 2) if elements else 0
            },
            'window_start': context.window().start,
            'window_end': context.window().end,
            'calculated_at': datetime.utcnow().isoformat()
        }
        
        yield json.dumps(result, ensure_ascii=False)


class ItemHotScoreCalculator:
    """ç‰©å“çƒ­åº¦å®æ—¶è®¡ç®—"""
    
    def __init__(
        self,
        kafka_servers: str = "localhost:9092",
        redis_url: str = "redis://:redis_password@localhost:6379/0"
    ):
        self.kafka_servers = kafka_servers
        self.redis_url = redis_url
        
        # è¡Œä¸ºæƒé‡
        self.action_weights = {
            'impression': 0.5,
            'view': 1.0,
            'click': 2.0,
            'like': 3.0,
            'favorite': 4.0,
            'share': 5.0,
            'comment': 4.0
        }
        
        # æ—¶é—´è¡°å‡ç³»æ•°
        self.decay_lambda = 0.1
    
    def run(self):
        """è¿è¡ŒFlinkä½œä¸š"""
        
        if not PYFLINK_AVAILABLE:
            print("=" * 60)
            print("  Flink Job: ç‰©å“çƒ­åº¦å®æ—¶è®¡ç®— (æ¨¡æ‹Ÿæ¨¡å¼)")
            print("=" * 60)
            print()
            print("ğŸ“Š ä½œä¸šé…ç½®:")
            print(f"  - Kafka: {self.kafka_servers}")
            print(f"  - Redis: {self.redis_url}")
            print(f"  - è¡Œä¸ºæƒé‡: {self.action_weights}")
            print(f"  - è¡°å‡ç³»æ•°: {self.decay_lambda}")
            print()
            print("âš ï¸  è¯·å®‰è£…PyFlink:")
            print("  pip install apache-flink")
            print()
            return
        
        print("=" * 60)
        print("  Flink Job: ç‰©å“çƒ­åº¦å®æ—¶è®¡ç®—")
        print("=" * 60)
        
        # 1. åˆ›å»ºæ‰§è¡Œç¯å¢ƒ
        env = StreamExecutionEnvironment.get_execution_environment()
        env.set_parallelism(4)
        
        # 2. ä»Kafkaè¯»å–ç”¨æˆ·è¡Œä¸ºæ•°æ®
        kafka_source = KafkaSource.builder() \
            .set_bootstrap_servers(self.kafka_servers) \
            .set_topics("user-behaviors-.*") \
            .set_group_id("item-hot-score-calculator") \
            .set_starting_offsets(KafkaOffsetsInitializer.latest()) \
            .set_value_only_deserializer(SimpleStringSchema()) \
            .build()
        
        behaviors = env.from_source(
            kafka_source,
            WatermarkStrategy.no_watermarks(),
            "Kafka User Behaviors Source"
        )
        
        # 3. è§£æJSON
        parsed_behaviors = behaviors.map(
            lambda x: json.loads(x),
            output_type=dict
        )
        
        # 4. æŒ‰ç‰©å“åˆ†ç»„ï¼Œæ»‘åŠ¨çª—å£èšåˆ
        hot_scores = (
            parsed_behaviors
            .key_by(lambda x: (x['tenant_id'], x['scenario_id'], x['item_id']))
            .window(SlidingProcessingTimeWindows.of(
                Time.hours(1),      # çª—å£å¤§å°: 1å°æ—¶
                Time.minutes(15)    # æ»‘åŠ¨æ­¥é•¿: 15åˆ†é’Ÿ
            ))
            .process(HotScoreCalculator(self.action_weights, self.decay_lambda))
        )
        
        # 5. å†™å…¥Redis ZSET
        def write_to_redis(score_json):
            """å†™å…¥Redisçš„å‡½æ•°"""
            try:
                import redis
                score_data = json.loads(score_json)
                
                r = redis.from_url(self.redis_url)
                
                # ZSET key: hot:items:{tenant_id}:{scenario_id}
                redis_key = f"hot:items:{score_data['tenant_id']}:{score_data['scenario_id']}"
                
                # æ·»åŠ åˆ°æœ‰åºé›†åˆ
                r.zadd(redis_key, {score_data['item_id']: score_data['hot_score']})
                
                # åªä¿ç•™Top 1000
                r.zremrangebyrank(redis_key, 0, -1001)
                
                # è®¾ç½®è¿‡æœŸæ—¶é—´ï¼ˆ2å°æ—¶ï¼‰
                r.expire(redis_key, 7200)
                
                print(f"[Redis] æ›´æ–°ç‰©å“çƒ­åº¦: {score_data['item_id']} = {score_data['hot_score']}")
                
            except Exception as e:
                print(f"[Redis] å†™å…¥å¤±è´¥: {e}")
        
        hot_scores.map(write_to_redis)
        
        # 6. æ‰“å°åˆ°æ§åˆ¶å°ï¼ˆè°ƒè¯•ç”¨ï¼‰
        hot_scores.print()
        
        # 7. æ‰§è¡Œä½œä¸š
        print("\nğŸš€ å¯åŠ¨Flinkä½œä¸š...")
        env.execute("Item Hot Score Real-time Calculator")


def main():
    """ä¸»å‡½æ•°"""
    import os
    
    kafka_servers = os.getenv('KAFKA_BOOTSTRAP_SERVERS', 'localhost:9092')
    redis_url = os.getenv('REDIS_URL', 'redis://:redis_password@localhost:6379/0')
    
    calculator = ItemHotScoreCalculator(
        kafka_servers=kafka_servers,
        redis_url=redis_url
    )
    
    try:
        calculator.run()
    except KeyboardInterrupt:
        print("\n\næ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œåœæ­¢ä½œä¸š...")
    except Exception as e:
        print(f"\n\nâŒ ä½œä¸šå¤±è´¥: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
