"""
Flink Job 1: ç”¨æˆ·ç”»åƒå®æ—¶æ›´æ–°
åŠŸèƒ½ï¼šä»Kafkaæ¶ˆè´¹ç”¨æˆ·è¡Œä¸ºæ•°æ®ï¼Œå®æ—¶èšåˆè®¡ç®—ç”¨æˆ·ç”»åƒï¼Œå†™å…¥MongoDBå’ŒRedis
"""
import json
import sys
from datetime import datetime
from collections import defaultdict

try:
    from pyflink.datastream import StreamExecutionEnvironment
    from pyflink.datastream.window import TumblingProcessingTimeWindows
    from pyflink.common import Time, WatermarkStrategy
    from pyflink.datastream.functions import ProcessWindowFunction, KeyedProcessFunction
    from pyflink.datastream.connectors.kafka import KafkaSource, KafkaOffsetsInitializer
    from pyflink.common.serialization import SimpleStringSchema
    PYFLINK_AVAILABLE = True
except ImportError:
    PYFLINK_AVAILABLE = False
    print("âš ï¸  PyFlinkæœªå®‰è£…ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼")


class UserProfileAggregator(ProcessWindowFunction):
    """ç”¨æˆ·ç”»åƒèšåˆå‡½æ•°"""
    
    def process(self, key, context, elements):
        """
        èšåˆçª—å£å†…çš„ç”¨æˆ·è¡Œä¸ºï¼Œç”Ÿæˆç”¨æˆ·ç”»åƒ
        
        Args:
            key: (tenant_id, user_id, scenario_id)
            context: çª—å£ä¸Šä¸‹æ–‡
            elements: çª—å£å†…çš„æ‰€æœ‰è¡Œä¸º
        """
        tenant_id, user_id, scenario_id = key
        
        # èšåˆç»Ÿè®¡
        stats = {
            'view_count': 0,
            'click_count': 0,
            'like_count': 0,
            'share_count': 0,
            'comment_count': 0,
            'favorite_count': 0,
            'categories': defaultdict(int),
            'tags': defaultdict(int),
            'active_hours': defaultdict(int),
            'viewed_items': []
        }
        
        for behavior in elements:
            action_type = behavior.get('action_type')
            
            # è¡Œä¸ºè®¡æ•°
            if action_type == 'view':
                stats['view_count'] += 1
            elif action_type == 'click':
                stats['click_count'] += 1
            elif action_type == 'like':
                stats['like_count'] += 1
            elif action_type == 'share':
                stats['share_count'] += 1
            elif action_type == 'comment':
                stats['comment_count'] += 1
            elif action_type == 'favorite':
                stats['favorite_count'] += 1
            
            # æ”¶é›†ç‰©å“ä¿¡æ¯
            item_id = behavior.get('item_id')
            if item_id:
                stats['viewed_items'].append(item_id)
            
            # åˆ†ç±»ç»Ÿè®¡ï¼ˆä»contextä¸­æå–ï¼‰
            context_data = behavior.get('context', {})
            if 'category' in context_data:
                stats['categories'][context_data['category']] += 1
            
            # æ ‡ç­¾ç»Ÿè®¡
            if 'tags' in context_data:
                for tag in context_data.get('tags', []):
                    stats['tags'][tag] += 1
            
            # æ´»è·ƒæ—¶æ®µ
            timestamp = behavior.get('timestamp')
            if timestamp:
                try:
                    dt = datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
                    hour = dt.hour
                    stats['active_hours'][hour] += 1
                except:
                    pass
        
        # è®¡ç®—åå¥½ï¼ˆTop 5ï¼‰
        top_categories = sorted(stats['categories'].items(), key=lambda x: x[1], reverse=True)[:5]
        top_tags = sorted(stats['tags'].items(), key=lambda x: x[1], reverse=True)[:10]
        
        # æ„é€ ç”¨æˆ·ç”»åƒ
        profile = {
            'tenant_id': tenant_id,
            'user_id': user_id,
            'scenario_id': scenario_id,
            'stats': {
                'view_count': stats['view_count'],
                'click_count': stats['click_count'],
                'like_count': stats['like_count'],
                'engagement_rate': stats['click_count'] / stats['view_count'] if stats['view_count'] > 0 else 0
            },
            'preferences': {
                'categories': [{'name': cat, 'score': score} for cat, score in top_categories],
                'tags': [{'name': tag, 'score': score} for tag, score in top_tags]
            },
            'active_hours': dict(stats['active_hours']),
            'recent_items': stats['viewed_items'][-20:],  # æœ€è¿‘20ä¸ª
            'window_start': context.window().start,
            'window_end': context.window().end,
            'updated_at': datetime.utcnow().isoformat()
        }
        
        yield json.dumps(profile, ensure_ascii=False)


class UserProfileUpdater:
    """ç”¨æˆ·ç”»åƒå®æ—¶æ›´æ–°"""
    
    def __init__(
        self,
        kafka_servers: str = "localhost:9092",
        mongodb_url: str = "mongodb://admin:password@localhost:27017",
        redis_url: str = "redis://:redis_password@localhost:6379/0"
    ):
        self.kafka_servers = kafka_servers
        self.mongodb_url = mongodb_url
        self.redis_url = redis_url
    
    def run(self):
        """è¿è¡ŒFlinkä½œä¸š"""
        
        if not PYFLINK_AVAILABLE:
            print("=" * 60)
            print("  Flink Job: ç”¨æˆ·ç”»åƒå®æ—¶æ›´æ–° (æ¨¡æ‹Ÿæ¨¡å¼)")
            print("=" * 60)
            print()
            print("ğŸ“Š ä½œä¸šé…ç½®:")
            print(f"  - Kafka: {self.kafka_servers}")
            print(f"  - MongoDB: {self.mongodb_url}")
            print(f"  - Redis: {self.redis_url}")
            print()
            print("âš ï¸  è¯·å®‰è£…PyFlink:")
            print("  pip install apache-flink")
            print()
            return
        
        print("=" * 60)
        print("  Flink Job: ç”¨æˆ·ç”»åƒå®æ—¶æ›´æ–°")
        print("=" * 60)
        
        # 1. åˆ›å»ºæ‰§è¡Œç¯å¢ƒ
        env = StreamExecutionEnvironment.get_execution_environment()
        env.set_parallelism(4)
        
        # 2. ä»Kafkaè¯»å–ç”¨æˆ·è¡Œä¸ºæ•°æ®
        kafka_source = KafkaSource.builder() \
            .set_bootstrap_servers(self.kafka_servers) \
            .set_topics("user-behaviors-.*")  \
            .set_group_id("user-profile-updater") \
            .set_starting_offsets(KafkaOffsetsInitializer.latest()) \
            .set_value_only_deserializer(SimpleStringSchema()) \
            .build()
        
        behaviors = env.from_source(
            kafka_source,
            WatermarkStrategy.no_watermarks(),
            "Kafka User Behaviors Source"
        )
        
        # 3. è§£æJSON
        parsed_behaviors = behaviors.map(
            lambda x: json.loads(x),
            output_type=dict
        )
        
        # 4. æŒ‰ç”¨æˆ·åˆ†ç»„å¹¶çª—å£èšåˆ
        user_profiles = (
            parsed_behaviors
            .key_by(lambda x: (x['tenant_id'], x['user_id'], x['scenario_id']))
            .window(TumblingProcessingTimeWindows.of(Time.minutes(5)))
            .process(UserProfileAggregator())
        )
        
        # 5. å†™å…¥MongoDB (ä½¿ç”¨è‡ªå®šä¹‰Sink)
        def write_to_mongodb(profile_json):
            """å†™å…¥MongoDBçš„å‡½æ•°"""
            try:
                from pymongo import MongoClient
                profile = json.loads(profile_json)
                
                client = MongoClient(self.mongodb_url)
                db = client['lemo_recommender']
                
                # æ›´æ–°æˆ–æ’å…¥
                db.user_profiles.update_one(
                    {
                        'tenant_id': profile['tenant_id'],
                        'user_id': profile['user_id'],
                        'scenario_id': profile['scenario_id']
                    },
                    {
                        '$set': profile,
                        '$inc': {'update_count': 1}
                    },
                    upsert=True
                )
                
                print(f"[MongoDB] æ›´æ–°ç”¨æˆ·ç”»åƒ: {profile['user_id']}")
                
            except Exception as e:
                print(f"[MongoDB] å†™å…¥å¤±è´¥: {e}")
        
        user_profiles.map(write_to_mongodb)
        
        # 6. å†™å…¥Redisç¼“å­˜
        def write_to_redis(profile_json):
            """å†™å…¥Redisçš„å‡½æ•°"""
            try:
                import redis
                profile = json.loads(profile_json)
                
                r = redis.from_url(self.redis_url)
                
                cache_key = f"user:profile:{profile['tenant_id']}:{profile['user_id']}:{profile['scenario_id']}"
                r.setex(cache_key, 3600, profile_json)  # 1å°æ—¶è¿‡æœŸ
                
                print(f"[Redis] ç¼“å­˜ç”¨æˆ·ç”»åƒ: {profile['user_id']}")
                
            except Exception as e:
                print(f"[Redis] å†™å…¥å¤±è´¥: {e}")
        
        user_profiles.map(write_to_redis)
        
        # 7. æ‰“å°åˆ°æ§åˆ¶å°ï¼ˆè°ƒè¯•ç”¨ï¼‰
        user_profiles.print()
        
        # 8. æ‰§è¡Œä½œä¸š
        print("\nğŸš€ å¯åŠ¨Flinkä½œä¸š...")
        env.execute("User Profile Real-time Updater")


def main():
    """ä¸»å‡½æ•°"""
    import os
    
    kafka_servers = os.getenv('KAFKA_BOOTSTRAP_SERVERS', 'localhost:9092')
    mongodb_url = os.getenv('MONGODB_URL', 'mongodb://admin:password@localhost:27017')
    redis_url = os.getenv('REDIS_URL', 'redis://:redis_password@localhost:6379/0')
    
    updater = UserProfileUpdater(
        kafka_servers=kafka_servers,
        mongodb_url=mongodb_url,
        redis_url=redis_url
    )
    
    try:
        updater.run()
    except KeyboardInterrupt:
        print("\n\næ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œåœæ­¢ä½œä¸š...")
    except Exception as e:
        print(f"\n\nâŒ ä½œä¸šå¤±è´¥: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
