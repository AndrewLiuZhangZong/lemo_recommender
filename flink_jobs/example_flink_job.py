"""
Flink ä½œä¸šç¤ºä¾‹è„šæœ¬
åŠŸèƒ½ï¼šä» Kafka æ¶ˆè´¹æ•°æ®ï¼Œè¿›è¡Œå®æ—¶å¤„ç†ï¼Œå†™å…¥ Redis/MongoDB
æ­¤è„šæœ¬å¯ä»¥ä½œä¸º Flink ä½œä¸šæ¨¡æ¿çš„ç¤ºä¾‹ä½¿ç”¨
"""
import json
import sys
import os
from datetime import datetime
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„ï¼ˆå¦‚æœéœ€è¦ä½¿ç”¨é¡¹ç›®ä¸­çš„æ¨¡å—ï¼‰
project_root = Path(__file__).parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

try:
    from pyflink.datastream import StreamExecutionEnvironment
    from pyflink.datastream.window import TumblingProcessingTimeWindows
    from pyflink.common import Time, WatermarkStrategy
    from pyflink.datastream.functions import ProcessWindowFunction
    from pyflink.datastream.connectors.kafka import KafkaSource, KafkaOffsetsInitializer
    from pyflink.common.serialization import SimpleStringSchema
    PYFLINK_AVAILABLE = True
except ImportError:
    PYFLINK_AVAILABLE = False
    print("âš ï¸  PyFlinkæœªå®‰è£…ï¼Œè¯·ç¡®ä¿ Flink Python ç¯å¢ƒå·²é…ç½®")


def main():
    """
    ä¸»å‡½æ•° - Flink ä½œä¸šå…¥å£ç‚¹
    
    æ­¤å‡½æ•°ä¼šè¢« Flink è°ƒç”¨ä½œä¸ºä½œä¸šçš„å…¥å£ç‚¹
    å¯ä»¥é€šè¿‡ args ä¼ é€’å‚æ•°ï¼Œä¾‹å¦‚ï¼š
    --tenant-id tenant1 --scenario-id scenario1 --kafka-topic user-behaviors
    """
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parse_args()
    
    print(f"ğŸš€ å¯åŠ¨ Flink ä½œä¸š: {args.get('job_name', 'Example Job')}")
    print(f"   å‚æ•°: {args}")
    
    # åˆ›å»º Flink æ‰§è¡Œç¯å¢ƒ
    env = StreamExecutionEnvironment.get_execution_environment()
    env.set_parallelism(args.get('parallelism', 1))
    
    # é…ç½® Checkpointï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if args.get('enable_checkpoint', False):
        env.enable_checkpointing(args.get('checkpoint_interval', 60000))
        print(f"   âœ“ Checkpoint å·²å¯ç”¨ï¼Œé—´éš”: {args.get('checkpoint_interval', 60000)}ms")
    
    # é…ç½® Kafka Source
    kafka_source = KafkaSource.builder() \
        .set_bootstrap_servers(args.get('kafka_bootstrap_servers', 'localhost:9092')) \
        .set_topics(args.get('kafka_topic', 'user-behaviors')) \
        .set_group_id(args.get('kafka_group_id', 'flink-job-group')) \
        .set_starting_offsets(KafkaOffsetsInitializer.latest()) \
        .set_value_only_deserializer(SimpleStringSchema()) \
        .build()
    
    # åˆ›å»ºæ•°æ®æµ
    data_stream = env.from_source(
        kafka_source,
        WatermarkStrategy.no_watermarks(),
        "Kafka Source"
    )
    
    # æ•°æ®å¤„ç†é€»è¾‘
    # ç¤ºä¾‹ï¼šå°† JSON å­—ç¬¦ä¸²è§£æå¹¶å¤„ç†
    processed_stream = data_stream \
        .map(lambda x: parse_and_process(x, args)) \
        .filter(lambda x: x is not None)
    
    # çª—å£èšåˆï¼ˆç¤ºä¾‹ï¼š5åˆ†é’Ÿæ»šåŠ¨çª—å£ï¼‰
    class WindowAggregator(ProcessWindowFunction):
        """çª—å£èšåˆå‡½æ•°ç±»"""
        def __init__(self, args):
            self.args = args
        
        def process(self, key, context, elements):
            # å°†å…ƒç´ è½¬æ¢ä¸ºåˆ—è¡¨å¹¶èšåˆ
            elements_list = list(elements)
            count = len(elements_list)
            
            result = {
                'key': str(key),
                'window_start': datetime.now().isoformat(),
                'count': count,
                'timestamp': datetime.now().isoformat()
            }
            
            print(f"ğŸ“Š çª—å£èšåˆç»“æœ - Key: {key}, Count: {count}")
            
            # ProcessWindowFunction éœ€è¦ yield ç»“æœ
            yield json.dumps(result)
    
    windowed_stream = processed_stream \
        .key_by(lambda x: x.get('key', 'default')) \
        .window(TumblingProcessingTimeWindows.of(Time.minutes(5))) \
        .process(WindowAggregator(args))
    
    # è¾“å‡ºåˆ° Redis/MongoDB/Kafkaï¼ˆæ ¹æ®é…ç½®ï¼‰
    output_type = args.get('output_type', 'print')  # print, redis, mongodb, kafka
    
    if output_type == 'print':
        # æ‰“å°è¾“å‡ºï¼ˆç”¨äºæµ‹è¯•ï¼‰
        windowed_stream.print("ç»“æœ")
    elif output_type == 'redis':
        # TODO: å®ç° Redis è¾“å‡º
        print("âš ï¸  Redis è¾“å‡ºåŠŸèƒ½å¾…å®ç°")
        windowed_stream.print("ç»“æœ")
    elif output_type == 'mongodb':
        # TODO: å®ç° MongoDB è¾“å‡º
        print("âš ï¸  MongoDB è¾“å‡ºåŠŸèƒ½å¾…å®ç°")
        windowed_stream.print("ç»“æœ")
    else:
        windowed_stream.print("ç»“æœ")
    
    # æ‰§è¡Œä½œä¸š
    print("ğŸ“Š ä½œä¸šå·²æäº¤ï¼Œç­‰å¾…æ‰§è¡Œ...")
    env.execute(args.get('job_name', 'Example Flink Job'))


def parse_args():
    """
    è§£æå‘½ä»¤è¡Œå‚æ•°
    
    è¿”å›å‚æ•°å­—å…¸
    """
    args = {}
    i = 1
    while i < len(sys.argv):
        if sys.argv[i].startswith('--'):
            key = sys.argv[i][2:]  # ç§»é™¤ '--'
            if i + 1 < len(sys.argv) and not sys.argv[i + 1].startswith('--'):
                args[key.replace('-', '_')] = sys.argv[i + 1]
                i += 2
            else:
                args[key.replace('-', '_')] = True
                i += 1
        else:
            i += 1
    
    # è®¾ç½®é»˜è®¤å€¼
    defaults = {
        'job_name': os.getenv('FLINK_JOB_NAME', 'Example Flink Job'),
        'parallelism': int(os.getenv('FLINK_PARALLELISM', '1')),
        'kafka_bootstrap_servers': os.getenv('KAFKA_BOOTSTRAP_SERVERS', 'localhost:9092'),
        'kafka_topic': os.getenv('KAFKA_TOPIC', 'user-behaviors'),
        'kafka_group_id': os.getenv('KAFKA_GROUP_ID', 'flink-job-group'),
        'enable_checkpoint': os.getenv('FLINK_ENABLE_CHECKPOINT', 'false').lower() == 'true',
        'checkpoint_interval': int(os.getenv('FLINK_CHECKPOINT_INTERVAL', '60000')),
        'output_type': os.getenv('OUTPUT_TYPE', 'print'),
    }
    
    # åˆå¹¶å‚æ•°ï¼Œå‘½ä»¤è¡Œå‚æ•°ä¼˜å…ˆçº§æ›´é«˜
    for key, value in defaults.items():
        if key not in args:
            args[key] = value
    
    return args


def parse_and_process(data_str, args):
    """
    è§£æå¹¶å¤„ç†å•æ¡æ•°æ®
    
    Args:
        data_str: JSON å­—ç¬¦ä¸²
        args: ä½œä¸šå‚æ•°
        
    Returns:
        å¤„ç†åçš„æ•°æ®å­—å…¸ï¼Œå¦‚æœè§£æå¤±è´¥è¿”å› None
    """
    try:
        data = json.loads(data_str)
        
        # ç¤ºä¾‹å¤„ç†é€»è¾‘ï¼šæ·»åŠ æ—¶é—´æˆ³
        data['processed_at'] = datetime.now().isoformat()
        
        # æå– key ç”¨äºåˆ†ç»„
        key = f"{data.get('tenant_id', 'default')}_{data.get('scenario_id', 'default')}"
        data['key'] = key
        
        return data
    except json.JSONDecodeError as e:
        print(f"âš ï¸  JSON è§£æå¤±è´¥: {e}, æ•°æ®: {data_str[:100]}")
        return None
    except Exception as e:
        print(f"âš ï¸  æ•°æ®å¤„ç†å¤±è´¥: {e}")
        return None


def aggregate_window(key, context, elements, args):
    """
    çª—å£èšåˆå‡½æ•°
    
    Args:
        key: çª—å£çš„ key
        context: çª—å£ä¸Šä¸‹æ–‡
        elements: çª—å£å†…çš„æ‰€æœ‰å…ƒç´ ï¼ˆè¿­ä»£å™¨ï¼‰
        args: ä½œä¸šå‚æ•°
        
    Returns:
        èšåˆç»“æœï¼ˆé€šè¿‡ yield è¿”å›ï¼‰
    """
    # å°†è¿­ä»£å™¨è½¬æ¢ä¸ºåˆ—è¡¨
    elements_list = list(elements) if hasattr(elements, '__iter__') else [elements]
    
    # ç¤ºä¾‹èšåˆé€»è¾‘ï¼šç»Ÿè®¡çª—å£å†…çš„æ•°æ®æ•°é‡
    count = len(elements_list)
    
    result = {
        'key': str(key),
        'window_start': datetime.now().isoformat(),
        'count': count,
        'timestamp': datetime.now().isoformat()
    }
    
    print(f"ğŸ“Š çª—å£èšåˆç»“æœ - Key: {key}, Count: {count}")
    
    # è¿”å›ç»“æœï¼ˆProcessWindowFunction éœ€è¦ yieldï¼‰
    yield json.dumps(result)


if __name__ == '__main__':
    main()

