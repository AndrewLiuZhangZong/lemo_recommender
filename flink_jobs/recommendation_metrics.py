"""
Flink Job 3: å®æ—¶æŒ‡æ ‡ç»Ÿè®¡ï¼ˆé…ç½®é©±åŠ¨ç‰ˆï¼‰
åŠŸèƒ½ï¼šä»Kafkaæ¶ˆè´¹ç”¨æˆ·è¡Œä¸ºæ•°æ®ï¼Œå®æ—¶è®¡ç®—æ¨èæ•ˆæœæŒ‡æ ‡ï¼Œè¾“å‡ºåˆ°Prometheus
ç‰¹æ€§ï¼šæ ¹æ® MongoDB Scenario é…ç½®åŠ¨æ€è®¡ç®—æŒ‡æ ‡
"""
import json
import sys
import os
import threading
import time
from datetime import datetime
from collections import defaultdict
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

try:
    from pyflink.datastream import StreamExecutionEnvironment
    from pyflink.datastream.window import TumblingProcessingTimeWindows
    from pyflink.common import Time, WatermarkStrategy
    from pyflink.datastream.functions import ProcessWindowFunction
    from pyflink.datastream.connectors.kafka import KafkaSource, KafkaOffsetsInitializer
    from pyflink.common.serialization import SimpleStringSchema
    PYFLINK_AVAILABLE = True
except ImportError:
    PYFLINK_AVAILABLE = False
    print("âš ï¸  PyFlinkæœªå®‰è£…ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼")

# å¯¼å…¥é…ç½®åŠ è½½å™¨
try:
    from app.services.realtime.config_loader import RealtimeConfigLoader
except ImportError:
    print("âš ï¸  æ— æ³•å¯¼å…¥ RealtimeConfigLoaderï¼Œè¯·æ£€æŸ¥é¡¹ç›®è·¯å¾„")
    RealtimeConfigLoader = None


class MetricsAggregator(ProcessWindowFunction):
    """æŒ‡æ ‡èšåˆå‡½æ•°ï¼ˆé…ç½®é©±åŠ¨ç‰ˆï¼‰"""
    
    def __init__(self, config_loader):
        """
        Args:
            config_loader: RealtimeConfigLoader å®ä¾‹ï¼Œç”¨äºåŠ¨æ€è·å–é…ç½®
        """
        self.config_loader = config_loader
    
    def process(self, key, context, elements):
        """
        èšåˆçª—å£å†…çš„è¡Œä¸ºæ•°æ®ï¼Œè®¡ç®—æ¨èæ•ˆæœæŒ‡æ ‡
        
        Args:
            key: (tenant_id, scenario_id)
            context: çª—å£ä¸Šä¸‹æ–‡
            elements: çª—å£å†…çš„æ‰€æœ‰è¡Œä¸º
        """
        tenant_id, scenario_id = key
        
        # ğŸ”¥ åŠ¨æ€è·å–é…ç½®ï¼ˆæ ¹æ®ç§Ÿæˆ·å’Œåœºæ™¯ï¼‰
        metrics_config = self.config_loader.get_metrics_config(tenant_id, scenario_id)
        metrics_to_calculate = metrics_config.get('metrics_to_calculate', ['ctr', 'cvr', 'watch_time'])
        
        # ç»Ÿè®¡è®¡æ•°å™¨
        stats = {
            'total_views': 0,
            'total_clicks': 0,
            'total_likes': 0,
            'total_shares': 0,
            'total_comments': 0,
            'total_duration': 0,
            'finished_views': 0,
            'unique_users': set(),
            'unique_items': set()
        }
        
        for behavior in elements:
            action_type = behavior.get('action_type')
            
            # åŸºç¡€è®¡æ•°
            if action_type == 'view':
                stats['total_views'] += 1
            elif action_type == 'click':
                stats['total_clicks'] += 1
            elif action_type == 'like':
                stats['total_likes'] += 1
            elif action_type == 'share':
                stats['total_shares'] += 1
            elif action_type == 'comment':
                stats['total_comments'] += 1
            
            # è§‚çœ‹æ—¶é•¿ç»Ÿè®¡
            extra = behavior.get('extra', {})
            if 'duration' in extra:
                try:
                    duration = float(extra['duration'])
                    stats['total_duration'] += duration
                    
                    # åˆ¤æ–­æ˜¯å¦å®Œæ’­ï¼ˆå‡è®¾>80%ä¸ºå®Œæ’­ï¼‰
                    if 'video_duration' in extra:
                        video_duration = float(extra['video_duration'])
                        if video_duration > 0 and duration / video_duration > 0.8:
                            stats['finished_views'] += 1
                except:
                    pass
            
            # å»é‡ç»Ÿè®¡
            stats['unique_users'].add(behavior.get('user_id'))
            stats['unique_items'].add(behavior.get('item_id'))
        
        # è®¡ç®—æŒ‡æ ‡
        total_views = stats['total_views']
        total_clicks = stats['total_clicks']
        
        metrics = {
            'tenant_id': tenant_id,
            'scenario_id': scenario_id,
            'window_start': context.window().start,
            'window_end': context.window().end,
            'metrics': {
                # ç‚¹å‡»ç‡
                'ctr': round(total_clicks / total_views, 4) if total_views > 0 else 0,
                
                # å¹³å‡è§‚çœ‹æ—¶é•¿ï¼ˆç§’ï¼‰
                'avg_watch_duration': round(stats['total_duration'] / total_views, 2) if total_views > 0 else 0,
                
                # å®Œæ’­ç‡
                'completion_rate': round(stats['finished_views'] / total_views, 4) if total_views > 0 else 0,
                
                # äº’åŠ¨ç‡ï¼ˆç‚¹èµ+åˆ†äº«+è¯„è®ºï¼‰
                'engagement_rate': round(
                    (stats['total_likes'] + stats['total_shares'] + stats['total_comments']) / total_views, 4
                ) if total_views > 0 else 0,
                
                # æ´»è·ƒç”¨æˆ·æ•°
                'active_users': len(stats['unique_users']),
                
                # æ›å…‰ç‰©å“æ•°
                'exposed_items': len(stats['unique_items']),
                
                # æ€»è¡Œä¸ºæ•°
                'total_actions': total_views + total_clicks + stats['total_likes'] + stats['total_shares']
            },
            'timestamp': datetime.utcnow().isoformat()
        }
        
        yield json.dumps(metrics, ensure_ascii=False)


class RecommendationMetrics:
    """æ¨èæ•ˆæœå®æ—¶æŒ‡æ ‡ç»Ÿè®¡ï¼ˆé…ç½®é©±åŠ¨ç‰ˆï¼‰"""
    
    def __init__(
        self,
        kafka_servers: str = "localhost:9092",
        mongodb_url: str = "mongodb://localhost:27017/",
        mongodb_database: str = "lemo_recommender",
        prometheus_pushgateway: str = "localhost:9091"
    ):
        self.kafka_servers = kafka_servers
        self.mongodb_url = mongodb_url
        self.mongodb_database = mongodb_database
        self.prometheus_pushgateway = prometheus_pushgateway
        
        # é…ç½®åŠ è½½å™¨
        if RealtimeConfigLoader:
            self.config_loader = RealtimeConfigLoader(mongodb_url, mongodb_database)
            print(f"âœ… é…ç½®åŠ è½½å™¨å·²åˆå§‹åŒ–")
            
            # åŠ è½½é…ç½®
            self.config_loader.load_configs()
            
            # å¯åŠ¨é…ç½®åˆ·æ–°çº¿ç¨‹
            self._start_config_refresh_thread()
        else:
            print("âš ï¸  RealtimeConfigLoader ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨é»˜è®¤é…ç½®")
            self.config_loader = None
    
    def _start_config_refresh_thread(self):
        """å¯åŠ¨é…ç½®åˆ·æ–°çº¿ç¨‹ï¼ˆæ¯5åˆ†é’Ÿåˆ·æ–°ä¸€æ¬¡ï¼‰"""
        def refresh():
            while True:
                try:
                    time.sleep(300)  # 5åˆ†é’Ÿ
                    self.config_loader.load_configs()
                    print("ğŸ”„ é…ç½®å·²åˆ·æ–°")
                except Exception as e:
                    print(f"âš ï¸  é…ç½®åˆ·æ–°å¤±è´¥: {e}")
        
        thread = threading.Thread(target=refresh, daemon=True)
        thread.start()
        print("âœ… é…ç½®åˆ·æ–°çº¿ç¨‹å·²å¯åŠ¨ï¼ˆæ¯5åˆ†é’Ÿï¼‰")
    
    def run(self):
        """è¿è¡ŒFlinkä½œä¸š"""
        
        if not PYFLINK_AVAILABLE:
            print("=" * 70)
            print("  Flink Job: æ¨èæŒ‡æ ‡å®æ—¶ç»Ÿè®¡ï¼ˆé…ç½®é©±åŠ¨ç‰ˆï¼‰ - æ¨¡æ‹Ÿæ¨¡å¼")
            print("=" * 70)
            print()
            print("ğŸ“Š ä½œä¸šé…ç½®:")
            print(f"  - Kafka: {self.kafka_servers}")
            print(f"  - MongoDB: {self.mongodb_url}")
            print(f"  - Database: {self.mongodb_database}")
            print(f"  - Prometheus: {self.prometheus_pushgateway}")
            if self.config_loader:
                print(f"  - å·²åŠ è½½åœºæ™¯æ•°: {len(self.config_loader.configs)}")
                self.config_loader.print_summary()
            print()
            print("âš ï¸  è¯·å®‰è£…PyFlink:")
            print("  pip install apache-flink")
            print()
            return
        
        print("=" * 70)
        print("  Flink Job: æ¨èæŒ‡æ ‡å®æ—¶ç»Ÿè®¡ï¼ˆé…ç½®é©±åŠ¨ç‰ˆï¼‰")
        print("=" * 70)
        
        # æ‰“å°é…ç½®æ‘˜è¦
        if self.config_loader:
            self.config_loader.print_summary()
        
        # 1. åˆ›å»ºæ‰§è¡Œç¯å¢ƒ
        env = StreamExecutionEnvironment.get_execution_environment()
        env.set_parallelism(4)
        
        # 2. ä»Kafkaè¯»å–ç”¨æˆ·è¡Œä¸ºæ•°æ®
        kafka_source = KafkaSource.builder() \
            .set_bootstrap_servers(self.kafka_servers) \
            .set_topics("user-behaviors-.*") \
            .set_group_id("recommendation-metrics") \
            .set_starting_offsets(KafkaOffsetsInitializer.latest()) \
            .set_value_only_deserializer(SimpleStringSchema()) \
            .build()
        
        behaviors = env.from_source(
            kafka_source,
            WatermarkStrategy.no_watermarks(),
            "Kafka User Behaviors Source"
        )
        
        # 3. è§£æJSON
        parsed_behaviors = behaviors.map(
            lambda x: json.loads(x),
            output_type=dict
        )
        
        # 4. æŒ‰åœºæ™¯åˆ†ç»„ï¼Œ1åˆ†é’Ÿçª—å£èšåˆï¼ˆä½¿ç”¨é…ç½®é©±åŠ¨çš„èšåˆå™¨ï¼‰
        metrics = (
            parsed_behaviors
            .key_by(lambda x: (x['tenant_id'], x['scenario_id']))
            .window(TumblingProcessingTimeWindows.of(Time.minutes(1)))
            .process(MetricsAggregator(self.config_loader))
        )
        
        # 5. æ¨é€åˆ°Prometheus Pushgateway
        def push_to_prometheus(metrics_json):
            """æ¨é€æŒ‡æ ‡åˆ°Prometheus"""
            try:
                import requests
                metrics_data = json.loads(metrics_json)
                
                tenant_id = metrics_data['tenant_id']
                scenario_id = metrics_data['scenario_id']
                m = metrics_data['metrics']
                
                # æ„é€ Prometheusæ ¼å¼çš„æŒ‡æ ‡
                prometheus_metrics = f"""
# TYPE recommendation_ctr gauge
recommendation_ctr{{tenant_id="{tenant_id}",scenario_id="{scenario_id}"}} {m['ctr']}

# TYPE recommendation_avg_duration gauge
recommendation_avg_duration{{tenant_id="{tenant_id}",scenario_id="{scenario_id}"}} {m['avg_watch_duration']}

# TYPE recommendation_completion_rate gauge
recommendation_completion_rate{{tenant_id="{tenant_id}",scenario_id="{scenario_id}"}} {m['completion_rate']}

# TYPE recommendation_engagement_rate gauge
recommendation_engagement_rate{{tenant_id="{tenant_id}",scenario_id="{scenario_id}"}} {m['engagement_rate']}

# TYPE recommendation_active_users gauge
recommendation_active_users{{tenant_id="{tenant_id}",scenario_id="{scenario_id}"}} {m['active_users']}

# TYPE recommendation_exposed_items gauge
recommendation_exposed_items{{tenant_id="{tenant_id}",scenario_id="{scenario_id}"}} {m['exposed_items']}
                """
                
                # æ¨é€åˆ°Pushgateway
                url = f"http://{self.prometheus_pushgateway}/metrics/job/recommendation_metrics/instance/{tenant_id}_{scenario_id}"
                response = requests.post(url, data=prometheus_metrics)
                
                if response.status_code == 200:
                    print(f"[Prometheus] æ¨é€æˆåŠŸ: {tenant_id}/{scenario_id}")
                else:
                    print(f"[Prometheus] æ¨é€å¤±è´¥: {response.status_code}")
                
            except Exception as e:
                print(f"[Prometheus] æ¨é€å¤±è´¥: {e}")
        
        metrics.map(push_to_prometheus)
        
        # 6. å†™å…¥Kafkaï¼ˆä¾›å…¶ä»–ç³»ç»Ÿæ¶ˆè´¹ï¼‰
        def send_to_kafka(metrics_json):
            """å‘é€åˆ°Kafka topic"""
            try:
                # è¿™é‡Œå¯ä»¥ä½¿ç”¨Kafka Producerå‘é€åˆ° recommendation-metrics topic
                print(f"[Kafka] å‘é€æŒ‡æ ‡: {metrics_json[:100]}...")
            except Exception as e:
                print(f"[Kafka] å‘é€å¤±è´¥: {e}")
        
        metrics.map(send_to_kafka)
        
        # 7. æ‰“å°åˆ°æ§åˆ¶å°ï¼ˆè°ƒè¯•ç”¨ï¼‰
        metrics.print()
        
        # 8. æ‰§è¡Œä½œä¸š
        print("\nğŸš€ å¯åŠ¨Flinkä½œä¸š...")
        env.execute("Recommendation Metrics Real-time")


def main():
    """ä¸»å‡½æ•°"""
    kafka_servers = os.getenv('KAFKA_BOOTSTRAP_SERVERS', 'localhost:9092')
    mongodb_url = os.getenv('MONGODB_URL', 'mongodb://localhost:27017/')
    mongodb_database = os.getenv('MONGODB_DATABASE', 'lemo_recommender')
    prometheus_pushgateway = os.getenv('PROMETHEUS_PUSHGATEWAY', 'localhost:9091')
    
    print("=" * 70)
    print("å¯åŠ¨å‚æ•°:")
    print(f"  - Kafka: {kafka_servers}")
    print(f"  - MongoDB: {mongodb_url}")
    print(f"  - Database: {mongodb_database}")
    print(f"  - Prometheus: {prometheus_pushgateway}")
    print("=" * 70)
    
    metrics = RecommendationMetrics(
        kafka_servers=kafka_servers,
        mongodb_url=mongodb_url,
        mongodb_database=mongodb_database,
        prometheus_pushgateway=prometheus_pushgateway
    )
    
    try:
        metrics.run()
    except KeyboardInterrupt:
        print("\n\næ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œåœæ­¢ä½œä¸š...")
    except Exception as e:
        print(f"\n\nâŒ ä½œä¸šå¤±è´¥: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
