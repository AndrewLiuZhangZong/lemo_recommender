"""
Flink Job 3: å®æ—¶æŒ‡æ ‡ç»Ÿè®¡
åŠŸèƒ½ï¼šä»Kafkaæ¶ˆè´¹ç”¨æˆ·è¡Œä¸ºæ•°æ®ï¼Œå®æ—¶è®¡ç®—æ¨èæ•ˆæœæŒ‡æ ‡ï¼Œè¾“å‡ºåˆ°Prometheus
"""
import json
import sys
from datetime import datetime
from collections import defaultdict

try:
    from pyflink.datastream import StreamExecutionEnvironment
    from pyflink.datastream.window import TumblingProcessingTimeWindows
    from pyflink.common import Time, WatermarkStrategy
    from pyflink.datastream.functions import ProcessWindowFunction
    from pyflink.datastream.connectors.kafka import KafkaSource, KafkaOffsetsInitializer
    from pyflink.common.serialization import SimpleStringSchema
    PYFLINK_AVAILABLE = True
except ImportError:
    PYFLINK_AVAILABLE = False
    print("âš ï¸  PyFlinkæœªå®‰è£…ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼")


class MetricsAggregator(ProcessWindowFunction):
    """æŒ‡æ ‡èšåˆå‡½æ•°"""
    
    def process(self, key, context, elements):
        """
        èšåˆçª—å£å†…çš„è¡Œä¸ºæ•°æ®ï¼Œè®¡ç®—æ¨èæ•ˆæœæŒ‡æ ‡
        
        Args:
            key: (tenant_id, scenario_id)
            context: çª—å£ä¸Šä¸‹æ–‡
            elements: çª—å£å†…çš„æ‰€æœ‰è¡Œä¸º
        """
        tenant_id, scenario_id = key
        
        # ç»Ÿè®¡è®¡æ•°å™¨
        stats = {
            'total_views': 0,
            'total_clicks': 0,
            'total_likes': 0,
            'total_shares': 0,
            'total_comments': 0,
            'total_duration': 0,
            'finished_views': 0,
            'unique_users': set(),
            'unique_items': set()
        }
        
        for behavior in elements:
            action_type = behavior.get('action_type')
            
            # åŸºç¡€è®¡æ•°
            if action_type == 'view':
                stats['total_views'] += 1
            elif action_type == 'click':
                stats['total_clicks'] += 1
            elif action_type == 'like':
                stats['total_likes'] += 1
            elif action_type == 'share':
                stats['total_shares'] += 1
            elif action_type == 'comment':
                stats['total_comments'] += 1
            
            # è§‚çœ‹æ—¶é•¿ç»Ÿè®¡
            extra = behavior.get('extra', {})
            if 'duration' in extra:
                try:
                    duration = float(extra['duration'])
                    stats['total_duration'] += duration
                    
                    # åˆ¤æ–­æ˜¯å¦å®Œæ’­ï¼ˆå‡è®¾>80%ä¸ºå®Œæ’­ï¼‰
                    if 'video_duration' in extra:
                        video_duration = float(extra['video_duration'])
                        if video_duration > 0 and duration / video_duration > 0.8:
                            stats['finished_views'] += 1
                except:
                    pass
            
            # å»é‡ç»Ÿè®¡
            stats['unique_users'].add(behavior.get('user_id'))
            stats['unique_items'].add(behavior.get('item_id'))
        
        # è®¡ç®—æŒ‡æ ‡
        total_views = stats['total_views']
        total_clicks = stats['total_clicks']
        
        metrics = {
            'tenant_id': tenant_id,
            'scenario_id': scenario_id,
            'window_start': context.window().start,
            'window_end': context.window().end,
            'metrics': {
                # ç‚¹å‡»ç‡
                'ctr': round(total_clicks / total_views, 4) if total_views > 0 else 0,
                
                # å¹³å‡è§‚çœ‹æ—¶é•¿ï¼ˆç§’ï¼‰
                'avg_watch_duration': round(stats['total_duration'] / total_views, 2) if total_views > 0 else 0,
                
                # å®Œæ’­ç‡
                'completion_rate': round(stats['finished_views'] / total_views, 4) if total_views > 0 else 0,
                
                # äº’åŠ¨ç‡ï¼ˆç‚¹èµ+åˆ†äº«+è¯„è®ºï¼‰
                'engagement_rate': round(
                    (stats['total_likes'] + stats['total_shares'] + stats['total_comments']) / total_views, 4
                ) if total_views > 0 else 0,
                
                # æ´»è·ƒç”¨æˆ·æ•°
                'active_users': len(stats['unique_users']),
                
                # æ›å…‰ç‰©å“æ•°
                'exposed_items': len(stats['unique_items']),
                
                # æ€»è¡Œä¸ºæ•°
                'total_actions': total_views + total_clicks + stats['total_likes'] + stats['total_shares']
            },
            'timestamp': datetime.utcnow().isoformat()
        }
        
        yield json.dumps(metrics, ensure_ascii=False)


class RecommendationMetrics:
    """æ¨èæ•ˆæœå®æ—¶æŒ‡æ ‡ç»Ÿè®¡"""
    
    def __init__(
        self,
        kafka_servers: str = "localhost:9092",
        prometheus_pushgateway: str = "localhost:9091"
    ):
        self.kafka_servers = kafka_servers
        self.prometheus_pushgateway = prometheus_pushgateway
    
    def run(self):
        """è¿è¡ŒFlinkä½œä¸š"""
        
        if not PYFLINK_AVAILABLE:
            print("=" * 60)
            print("  Flink Job: æ¨èæŒ‡æ ‡å®æ—¶ç»Ÿè®¡ (æ¨¡æ‹Ÿæ¨¡å¼)")
            print("=" * 60)
            print()
            print("ğŸ“Š ä½œä¸šé…ç½®:")
            print(f"  - Kafka: {self.kafka_servers}")
            print(f"  - Prometheus: {self.prometheus_pushgateway}")
            print()
            print("âš ï¸  è¯·å®‰è£…PyFlink:")
            print("  pip install apache-flink")
            print()
            return
        
        print("=" * 60)
        print("  Flink Job: æ¨èæŒ‡æ ‡å®æ—¶ç»Ÿè®¡")
        print("=" * 60)
        
        # 1. åˆ›å»ºæ‰§è¡Œç¯å¢ƒ
        env = StreamExecutionEnvironment.get_execution_environment()
        env.set_parallelism(4)
        
        # 2. ä»Kafkaè¯»å–ç”¨æˆ·è¡Œä¸ºæ•°æ®
        kafka_source = KafkaSource.builder() \
            .set_bootstrap_servers(self.kafka_servers) \
            .set_topics("user-behaviors-.*") \
            .set_group_id("recommendation-metrics") \
            .set_starting_offsets(KafkaOffsetsInitializer.latest()) \
            .set_value_only_deserializer(SimpleStringSchema()) \
            .build()
        
        behaviors = env.from_source(
            kafka_source,
            WatermarkStrategy.no_watermarks(),
            "Kafka User Behaviors Source"
        )
        
        # 3. è§£æJSON
        parsed_behaviors = behaviors.map(
            lambda x: json.loads(x),
            output_type=dict
        )
        
        # 4. æŒ‰åœºæ™¯åˆ†ç»„ï¼Œ1åˆ†é’Ÿçª—å£èšåˆ
        metrics = (
            parsed_behaviors
            .key_by(lambda x: (x['tenant_id'], x['scenario_id']))
            .window(TumblingProcessingTimeWindows.of(Time.minutes(1)))
            .process(MetricsAggregator())
        )
        
        # 5. æ¨é€åˆ°Prometheus Pushgateway
        def push_to_prometheus(metrics_json):
            """æ¨é€æŒ‡æ ‡åˆ°Prometheus"""
            try:
                import requests
                metrics_data = json.loads(metrics_json)
                
                tenant_id = metrics_data['tenant_id']
                scenario_id = metrics_data['scenario_id']
                m = metrics_data['metrics']
                
                # æ„é€ Prometheusæ ¼å¼çš„æŒ‡æ ‡
                prometheus_metrics = f"""
# TYPE recommendation_ctr gauge
recommendation_ctr{{tenant_id="{tenant_id}",scenario_id="{scenario_id}"}} {m['ctr']}

# TYPE recommendation_avg_duration gauge
recommendation_avg_duration{{tenant_id="{tenant_id}",scenario_id="{scenario_id}"}} {m['avg_watch_duration']}

# TYPE recommendation_completion_rate gauge
recommendation_completion_rate{{tenant_id="{tenant_id}",scenario_id="{scenario_id}"}} {m['completion_rate']}

# TYPE recommendation_engagement_rate gauge
recommendation_engagement_rate{{tenant_id="{tenant_id}",scenario_id="{scenario_id}"}} {m['engagement_rate']}

# TYPE recommendation_active_users gauge
recommendation_active_users{{tenant_id="{tenant_id}",scenario_id="{scenario_id}"}} {m['active_users']}

# TYPE recommendation_exposed_items gauge
recommendation_exposed_items{{tenant_id="{tenant_id}",scenario_id="{scenario_id}"}} {m['exposed_items']}
                """
                
                # æ¨é€åˆ°Pushgateway
                url = f"http://{self.prometheus_pushgateway}/metrics/job/recommendation_metrics/instance/{tenant_id}_{scenario_id}"
                response = requests.post(url, data=prometheus_metrics)
                
                if response.status_code == 200:
                    print(f"[Prometheus] æ¨é€æˆåŠŸ: {tenant_id}/{scenario_id}")
                else:
                    print(f"[Prometheus] æ¨é€å¤±è´¥: {response.status_code}")
                
            except Exception as e:
                print(f"[Prometheus] æ¨é€å¤±è´¥: {e}")
        
        metrics.map(push_to_prometheus)
        
        # 6. å†™å…¥Kafkaï¼ˆä¾›å…¶ä»–ç³»ç»Ÿæ¶ˆè´¹ï¼‰
        def send_to_kafka(metrics_json):
            """å‘é€åˆ°Kafka topic"""
            try:
                # è¿™é‡Œå¯ä»¥ä½¿ç”¨Kafka Producerå‘é€åˆ° recommendation-metrics topic
                print(f"[Kafka] å‘é€æŒ‡æ ‡: {metrics_json[:100]}...")
            except Exception as e:
                print(f"[Kafka] å‘é€å¤±è´¥: {e}")
        
        metrics.map(send_to_kafka)
        
        # 7. æ‰“å°åˆ°æ§åˆ¶å°ï¼ˆè°ƒè¯•ç”¨ï¼‰
        metrics.print()
        
        # 8. æ‰§è¡Œä½œä¸š
        print("\nğŸš€ å¯åŠ¨Flinkä½œä¸š...")
        env.execute("Recommendation Metrics Real-time")


def main():
    """ä¸»å‡½æ•°"""
    import os
    
    kafka_servers = os.getenv('KAFKA_BOOTSTRAP_SERVERS', 'localhost:9092')
    prometheus_pushgateway = os.getenv('PROMETHEUS_PUSHGATEWAY', 'localhost:9091')
    
    metrics = RecommendationMetrics(
        kafka_servers=kafka_servers,
        prometheus_pushgateway=prometheus_pushgateway
    )
    
    try:
        metrics.run()
    except KeyboardInterrupt:
        print("\n\næ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œåœæ­¢ä½œä¸š...")
    except Exception as e:
        print(f"\n\nâŒ ä½œä¸šå¤±è´¥: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
