# 推荐系统优化计划 v2.0 - 服务拆分与架构升级

> 🎯 **目标**: 对标抖音/快手，支持2亿+DAU的超大规模推荐系统
> 
> 📅 **规划日期**: 2024-11-05  
> 📋 **前置依赖**: v1.0性能优化（已完成）  
> 🏗️ **部署环境**: Kubernetes（K8s）集群

---

## 📊 当前架构 vs 目标架构

### 当前架构（4个服务）

```
当前部署：
├── HTTP API（1副本）
├── gRPC（1副本）
├── Worker（1副本）- 部署脚本已有 ✅
└── Beat（1副本）- 部署脚本已有 ✅

功能覆盖：95%
QPS支持：~5K
适用规模：日活 < 100万
```

### 目标架构（13个服务）

```
2亿+用户架构：
├── 在线服务层（6个服务）
│   ├── 推荐查询服务（召回+粗排）
│   ├── 精排服务（DeepFM/双塔）
│   ├── 重排服务（多样性/业务规则）
│   ├── 用户服务（画像查询）
│   ├── 物品服务（内容查询）
│   └── 行为服务（埋点采集）
├── 离线计算层（4个服务）
│   ├── 模型训练服务（GPU集群）
│   ├── 特征工程服务（Spark/Flink）
│   ├── 向量生成服务（Embedding）
│   └── 数据同步服务（Worker队列拆分）
└── 实时计算层（3个服务）
    ├── Flink实时特征服务
    ├── 实时推荐流服务
    └── Beat定时任务调度

说明：
- ✅ 网关层由外部网关提供（已有）
- ✅ 配置管理通过K8s ConfigMap/Secret
- ✅ 服务发现通过K8s Service
- ✅ 负载均衡通过K8s Service/Ingress

QPS支持：100K+
适用规模：日活 2亿+
```

---

## 🎯 核心服务拆分设计

### 阶段1：在线服务拆分（P0 - 立即实施）

#### 1.1 推荐服务拆分为3个独立服务

**原因**：
- 召回/排序/重排有不同的性能特征
- 独立扩缩容，资源利用率提升50%
- 灰度发布更安全

##### 服务1: 召回服务（Recall Service）

**职责**：
- 多路召回（ALS、UserCF、ItemCF、Hot、Vector）
- 召回结果合并去重
- 布隆过滤器去重

**性能指标**：
- QPS: 10K+
- P99延迟: < 50ms
- 召回数量: 500-1000个候选

**部署配置**：
```yaml
# k8s-deployment-recall-service.yaml
replicas: 10  # 召回服务，高并发
resources:
  requests:
    cpu: 500m
    memory: 1Gi
  limits:
    cpu: 2000m
    memory: 4Gi
autoscaling:
  minReplicas: 10
  maxReplicas: 50
  targetCPUUtilizationPercentage: 70
```

**新增文件**：
- `app/services/recall_service/main.py`
- `k8s-deploy/k8s-deployment-recall-service.yaml`
- `k8s-deploy/deploy-recall-service.sh`

---

##### 服务2: 精排服务（Ranking Service）

**职责**：
- DeepFM/双塔模型推理
- 批量预测优化
- ONNX加速
- GPU推理（可选）

**性能指标**：
- QPS: 10K+
- P99延迟: < 30ms
- 批量大小: 100-500

**部署配置**：
```yaml
# k8s-deployment-ranking-service.yaml
replicas: 8
resources:
  requests:
    cpu: 1000m
    memory: 2Gi
  limits:
    cpu: 4000m
    memory: 8Gi
# GPU节点（可选）
nodeSelector:
  nvidia.com/gpu: "true"
```

**新增文件**：
- `app/services/ranking_service/main.py`
- `k8s-deploy/k8s-deployment-ranking-service.yaml`
- `k8s-deploy/deploy-ranking-service.sh`

---

##### 服务3: 重排服务（Reranking Service）

**职责**：
- 多样性重排
- 业务规则过滤
- A/B实验分流
- 打散去重

**性能指标**：
- QPS: 10K+
- P99延迟: < 10ms

**部署配置**：
```yaml
replicas: 5
resources:
  requests:
    cpu: 200m
    memory: 512Mi
  limits:
    cpu: 1000m
    memory: 2Gi
```

**新增文件**：
- `app/services/reranking_service/main.py`
- `k8s-deploy/k8s-deployment-reranking-service.yaml`

---

#### 1.2 用户服务独立拆分

**职责**：
- 用户画像查询
- 用户实时特征（Redis）
- 用户偏好标签

**为什么拆分**：
- 用户数据量大（2亿用户）
- 需要独立缓存和存储优化
- 与推荐服务解耦

**部署配置**：
```yaml
replicas: 6
resources:
  requests:
    cpu: 500m
    memory: 2Gi
```

**新增文件**：
- `app/services/user_service/main.py`

---

#### 1.3 物品服务独立拆分

**职责**：
- 物品元数据查询
- 物品向量查询（Milvus）
- 物品实时热度

**为什么拆分**：
- 物品数据量大（千万级）
- 需要独立索引和缓存
- 支持多场景复用

**部署配置**：
```yaml
replicas: 8
resources:
  requests:
    cpu: 500m
    memory: 2Gi
```

---

#### 1.4 行为服务独立拆分

**职责**：
- 行为埋点接收（高吞吐）
- 写入Kafka
- 实时去重

**为什么拆分**：
- 流量巨大（写多读少）
- 需要独立扩容
- 避免影响推荐查询

**部署配置**：
```yaml
replicas: 10
resources:
  requests:
    cpu: 200m
    memory: 512Mi
```

---

### 阶段2：离线计算拆分（P1）

#### 2.1 模型训练服务（独立GPU集群）

**职责**：
- DeepFM/双塔/多任务模型训练
- 自动调参（AutoML）
- 模型版本管理

**部署**：
```yaml
# 使用单独的GPU节点池
nodeSelector:
  node-type: gpu-training
resources:
  limits:
    nvidia.com/gpu: 4  # 每个Pod 4张GPU
replicas: 2-4  # 按需调度
```

**新增文件**：
- `app/services/training_service/main.py`

---

#### 2.2 特征工程服务

**职责**：
- Spark任务调度
- 特征计算和存储
- 特征实时更新

**技术栈**：
- Spark on Kubernetes
- Flink SQL

---

#### 2.3 向量生成服务

**职责**：
- 物品向量生成
- 用户向量生成
- 向量索引构建（HNSW）

---

### 阶段3：实时计算拆分（P1）

#### 3.1 Flink实时特征服务

**职责**：
- 实时特征计算（窗口聚合）
- 用户实时画像更新
- 物品实时热度计算

**部署**：
- Flink Kubernetes Operator
- TaskManager: 10-20个
- 自动伸缩

---

#### 3.2 实时推荐流服务

**职责**：
- 流式推荐（feed流）
- 实时个性化
- 冷启动处理

---

### 阶段4：K8s基础设施优化（P2）

#### 4.1 K8s服务配置

**使用K8s原生能力**：
- ✅ ConfigMap/Secret管理配置
- ✅ Service实现服务发现
- ✅ Ingress实现流量路由
- ✅ HPA实现自动伸缩
- ✅ NetworkPolicy实现网络隔离

**配置示例**：
```yaml
# 使用K8s Service进行服务发现
apiVersion: v1
kind: Service
metadata:
  name: recall-service
spec:
  selector:
    app: recall-service
  ports:
  - port: 8080
    targetPort: 8080

# 其他服务通过DNS访问
# http://recall-service.lemo-dev.svc.cluster.local:8080
```

#### 4.2 服务间通信

**gRPC + K8s Service**：
```python
# 服务间调用示例
import grpc

# 通过K8s Service DNS直接访问
channel = grpc.insecure_channel(
    'recall-service.lemo-dev.svc.cluster.local:50051'
)
stub = RecallServiceStub(channel)
```

**优势**：
- ✅ 无需额外组件
- ✅ K8s原生支持
- ✅ 配置简单
- ✅ 性能开销低

---

## 📊 资源规划（2亿+用户）

### 服务资源清单

| 服务 | 副本数 | CPU | 内存 | 总CPU | 总内存 |
|------|--------|-----|------|-------|--------|
| **召回服务** | 10 | 2 | 4Gi | 20 | 40Gi |
| **精排服务** | 8 | 4 | 8Gi | 32 | 64Gi |
| **重排服务** | 5 | 1 | 2Gi | 5 | 10Gi |
| **用户服务** | 6 | 0.5 | 2Gi | 3 | 12Gi |
| **物品服务** | 8 | 0.5 | 2Gi | 4 | 16Gi |
| **行为服务** | 10 | 0.2 | 0.5Gi | 2 | 5Gi |
| **Worker** | 20 | 2 | 4Gi | 40 | 80Gi |
| **Beat** | 1 | 0.2 | 0.5Gi | 0.2 | 0.5Gi |
| **Flink** | 15 | 2 | 4Gi | 30 | 60Gi |
| **模型训练** | 2 | 8+4GPU | 16Gi | 16 | 32Gi |
| **合计** | **85** | - | - | **152核** | **320Gi** |

**成本估算**：
- K8s集群：40-50台32核64G服务器
- GPU集群：2-4台8卡A100服务器
- 月成本：约50-80万元（云服务）

---

## 🚀 实施路线图

### Phase 0: 当前架构（已完成 ✅）

```
服务数：4个
├── HTTP API ✅
├── gRPC ✅
├── Worker ✅
└── Beat ✅

适用规模：日活 < 100万
```

---

### Phase 1: 核心服务拆分（Week 1-2）⭐ 优先实施

```
服务数：7个（+3）
├── HTTP API（保留作为编排层）
├── gRPC（保留作为内部通信）
├── ➕ 召回服务（新增，独立K8s Service）
├── ➕ 精排服务（新增，独立K8s Service）
├── ➕ 重排服务（新增，独立K8s Service）
├── Worker
└── Beat

K8s配置：
- 每个服务独立Deployment
- 通过K8s Service实现服务发现
- 通过Ingress实现外部访问（由外部网关处理）

适用规模：日活 100万-1000万
预计QPS：10K+
```

**实施步骤**：
1. 创建召回服务独立代码目录
2. 编写召回服务K8s部署配置
3. 修改HTTP API为编排服务（BFF模式）
4. 灰度发布，逐步切流量

---

### Phase 2: 数据服务拆分（Week 3-4）

```
服务数：10个（+3）
├── HTTP API（编排层）
├── 召回服务
├── 精排服务
├── 重排服务
├── ➕ 用户服务（新增，独立K8s Service）
├── ➕ 物品服务（新增，独立K8s Service）
├── ➕ 行为服务（新增，独立K8s Service）
├── Worker
├── Beat
└── Consumer

K8s架构：
- 所有服务通过K8s Service DNS互相访问
- 使用K8s NetworkPolicy实现网络隔离
- 配置统一存储在K8s ConfigMap

适用规模：日活 1000万-5000万
预计QPS：30K+
```

---

### Phase 3: 计算服务拆分（Week 5-8）

```
服务数：15个（+5）
├── API Gateway
├── 召回服务
├── 精排服务
├── 重排服务
├── 用户服务
├── 物品服务
├── 行为服务
├── Worker（多队列拆分）
│   ├── ➕ 模型训练Worker
│   ├── ➕ 向量生成Worker
│   └── ➕ 数据处理Worker
├── Beat
├── Consumer
├── ➕ Flink实时特征服务
├── ➕ 实时推荐流服务
└── ➕ 特征工程服务

适用规模：日活 5000万-2亿
预计QPS：50K-100K+
```

---

### Phase 4: 完整架构（Week 9-12）

```
服务数：13个
├── 完整微服务架构（基于K8s）
├── 多机房部署（K8s集群联邦）
├── 边缘计算节点（Edge K8s）
└── 全球CDN（由外部网关处理）

K8s高可用配置：
- 多副本部署
- HPA自动伸缩
- PodDisruptionBudget保证可用性
- 跨可用区部署

适用规模：日活 2亿+
预计QPS：100K+
```

---

## 💡 关键技术点

### 1. 服务通信优化（基于K8s）

```
服务间调用链路（通过K8s Service DNS）：
HTTP API → recall-service.lemo-dev.svc → ranking-service.lemo-dev.svc → reranking-service.lemo-dev.svc
    ↓              ↓                            ↓
user-service   item-service              behavior-service

K8s原生能力：
- Service DNS自动服务发现
- Service负载均衡（iptables/ipvs）
- Headless Service支持直连Pod
- NetworkPolicy网络隔离

优化方案：
- gRPC通信（比HTTP快30%）
- 批量请求合并
- 异步并行调用
- 本地缓存
```

### 2. 数据库拆分

```
当前：所有服务共享MongoDB
目标：按服务拆分数据库

用户服务：
├── MongoDB（用户画像）
├── Redis（实时特征）
└── ClickHouse（行为日志）

物品服务：
├── MongoDB（物品元数据）
├── Milvus（物品向量）
└── Redis（热度缓存）
```

### 3. 缓存架构

```
L1（服务本地缓存）: 1ms
  ↓ 未命中
L2（Redis集群缓存）: 5ms
  ↓ 未命中
L3（数据库查询）: 50ms
```

---

## 📊 性能对比

| 指标 | 当前架构 | Phase 1 | Phase 3 | 提升 |
|------|---------|---------|---------|------|
| **推荐延迟P99** | 100ms | 60ms | 30ms | 70% ⬇️ |
| **QPS支持** | 5K | 10K | 100K | 1900% ⬆️ |
| **并发用户** | 10万 | 100万 | 2亿+ | 2000倍 |
| **服务可用性** | 99% | 99.9% | 99.99% | - |
| **故障隔离** | 差 | 中 | 优 | - |

---

## ✅ 下一步行动计划

### 立即开始（本周）

1. **创建服务目录结构**
```bash
mkdir -p app/services/recall_service
mkdir -p app/services/ranking_service
mkdir -p app/services/reranking_service
```

2. **编写召回服务**
```python
# app/services/recall_service/main.py
# 独立的FastAPI服务，只负责召回
```

3. **编写K8s部署配置**
```yaml
# k8s-deploy/k8s-deployment-recall-service.yaml
```

4. **修改HTTP API为BFF模式**
```python
# HTTP API变为编排层，调用召回/精排/重排服务
```

### 需要我帮你实现吗？

我可以立即开始帮你：
1. ✅ 创建召回服务的完整代码
2. ✅ 编写K8s部署配置和脚本
3. ✅ 修改现有HTTP API为BFF模式
4. ✅ 提供完整的实施文档

**是否开始实施Phase 1的服务拆分？**🚀

---

## 📦 K8s部署最佳实践

### 1. 服务发现配置

**所有服务通过K8s Service DNS访问**：

```yaml
# 召回服务Service
apiVersion: v1
kind: Service
metadata:
  name: recall-service
  namespace: lemo-dev
spec:
  selector:
    app: recall-service
  ports:
  - name: grpc
    port: 50051
    targetPort: 50051
  - name: http
    port: 8080
    targetPort: 8080
  type: ClusterIP
```

**服务间调用示例**：
```python
# 在任意Pod中访问召回服务
import grpc

# 通过K8s Service DNS
channel = grpc.insecure_channel(
    'recall-service.lemo-dev.svc.cluster.local:50051'
)

# 简写形式（同namespace）
channel = grpc.insecure_channel('recall-service:50051')
```

---

### 2. 配置管理

**使用K8s ConfigMap统一管理**：

```yaml
# lemo-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: lemo-config
  namespace: lemo-dev
data:
  # 数据库配置
  MONGODB_URL: "mongodb://..."
  REDIS_URL: "redis://..."
  
  # 服务地址（使用K8s Service DNS）
  RECALL_SERVICE_URL: "recall-service.lemo-dev.svc:50051"
  RANKING_SERVICE_URL: "ranking-service.lemo-dev.svc:50051"
  RERANKING_SERVICE_URL: "reranking-service.lemo-dev.svc:50051"
  USER_SERVICE_URL: "user-service.lemo-dev.svc:50051"
  ITEM_SERVICE_URL: "item-service.lemo-dev.svc:50051"
```

**Pod中引用ConfigMap**：
```yaml
spec:
  containers:
  - name: app
    envFrom:
    - configMapRef:
        name: lemo-config
```

---

### 3. 自动伸缩配置

**使用HPA实现自动扩缩容**：

```yaml
# recall-service-hpa.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: recall-service-hpa
  namespace: lemo-dev
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: recall-service
  minReplicas: 5
  maxReplicas: 20
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
```

---

### 4. 网络隔离

**使用NetworkPolicy实现安全隔离**：

```yaml
# network-policy.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: recall-service-policy
  namespace: lemo-dev
spec:
  podSelector:
    matchLabels:
      app: recall-service
  policyTypes:
  - Ingress
  ingress:
  # 只允许来自HTTP API的访问
  - from:
    - podSelector:
        matchLabels:
          app: lemo-service-recommender-http
    ports:
    - protocol: TCP
      port: 50051
```

---

### 5. 健康检查

**配置Liveness和Readiness探针**：

```yaml
spec:
  containers:
  - name: recall-service
    livenessProbe:
      grpc:
        port: 50051
      initialDelaySeconds: 30
      periodSeconds: 10
      timeoutSeconds: 5
      failureThreshold: 3
    
    readinessProbe:
      grpc:
        port: 50051
      initialDelaySeconds: 10
      periodSeconds: 5
      timeoutSeconds: 3
```

---

### 6. 资源限制

**合理配置资源请求和限制**：

```yaml
spec:
  containers:
  - name: recall-service
    resources:
      requests:
        cpu: "2"
        memory: "4Gi"
      limits:
        cpu: "4"
        memory: "8Gi"
```

---

### 7. 部署策略

**使用滚动更新确保零停机**：

```yaml
spec:
  replicas: 10
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 3        # 最多多3个Pod
      maxUnavailable: 1  # 最多停1个Pod
  minReadySeconds: 10    # Pod就绪后等10秒
```

---

### 8. 优先级和抢占

**为核心服务设置高优先级**：

```yaml
# priority-class.yaml
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: high-priority
value: 1000
globalDefault: false
description: "高优先级服务（召回、精排）"

---
# 在Deployment中使用
spec:
  template:
    spec:
      priorityClassName: high-priority
```

---

### 9. 亲和性配置

**将服务分散到不同节点**：

```yaml
spec:
  template:
    spec:
      # Pod反亲和性（避免单点故障）
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  app: recall-service
              topologyKey: kubernetes.io/hostname
```

---

### 10. 外部访问（通过你的网关）

**使用Ingress暴露服务**：

```yaml
# ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: lemo-ingress
  namespace: lemo-dev
  annotations:
    # 由你的外部网关处理
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
  - host: api.lemo.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: lemo-service-recommender-http
            port:
              number: 8080
```

---

## 📋 K8s部署检查清单

部署前确认：

- [ ] 所有服务都有独立的Deployment
- [ ] 所有服务都有对应的Service（用于服务发现）
- [ ] 配置统一存储在ConfigMap/Secret
- [ ] 资源requests/limits合理配置
- [ ] HPA配置完成（核心服务）
- [ ] 健康检查配置完成
- [ ] NetworkPolicy配置完成（如需隔离）
- [ ] 服务间通过K8s Service DNS访问
- [ ] 外部访问通过Ingress + 你的网关
- [ ] 日志收集配置完成
- [ ] 监控指标暴露（Prometheus）

---

## 🎯 与v1.0的关系

| 版本 | 目标 | 服务数 | 适用规模 |
|------|------|--------|---------|
| **v1.0** | 性能优化 | 4个 | 日活 < 100万 |
| **v2.0** | 架构升级 | 13个 | 日活 2亿+ |

**依赖关系**：
- v2.0基于v1.0的优化成果（缓存、并行召回、ALS、DeepFM）
- v2.0将v1.0的优化点拆分到独立服务
- 两者是**递进关系**，不是替代关系

**建议**：
1. 先完成v1.0的性能优化（已完成✅）
2. 验证v1.0效果稳定后，再实施v2.0
3. v2.0分Phase逐步实施，降低风险

---

**准备好开始实施了吗？** 🚀

