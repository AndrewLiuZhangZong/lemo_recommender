# 埋点接入完整指南

本文档提供完整的埋点数据接入方案，包括HTTP、gRPC和Kafka三种方式，以及Proto代码生成指南。

---

## 📋 目录

1. [埋点概述](#埋点概述)
2. [支持的场景类型](#支持的场景类型)
3. [接入方式选择](#接入方式选择)
4. [HTTP/gRPC接入方式](#httpgrpc接入方式)
5. [Kafka直接接入方式](#kafka直接接入方式)
6. [Proto代码生成指南](#proto代码生成指南)
7. [配置管理](#配置管理)
8. [验证和调试](#验证和调试)
9. [最佳实践](#最佳实践)
10. [常见问题](#常见问题)

---

## 埋点概述

### 什么是场景埋点？

场景埋点是指针对不同业务场景（视频、电商、新闻等）定义特定的数据采集字段和验证规则。通过场景埋点配置，可以：

✅ **标准化数据采集** - 统一场景内的数据字段  
✅ **自动数据验证** - 确保数据质量和完整性  
✅ **灵活扩展** - 支持自定义场景和字段  
✅ **提升推荐效果** - 采集更精准的场景特征

### 架构设计

```
C端应用 → HTTP/gRPC API → behavior-service → Kafka → Flink → ClickHouse
                               ↓
内部服务 → Kafka直接写入 ──→ Kafka → Flink → ClickHouse
                               ↓
                        场景埋点配置(MongoDB)
```

---

## 支持的场景类型

### 1. 视频场景 (video)

适用于：短视频、vlog、视频课程等

**核心字段：**
- `duration`: 视频总时长（秒）
- `watch_duration`: 观看时长（秒）
- `completion_rate`: 完成率（0-1）
- `video_quality`: 视频质量（360p/720p/1080p/4K）
- `is_fullscreen`: 是否全屏
- `playback_speed`: 播放速度（1.0/1.5/2.0）

**推荐行为：** impression, click, play, play_end, pause, like, share, comment

### 2. 电商场景 (ecommerce)

适用于：商品推荐、购物车、订单等

**核心字段：**
- `price`: 商品价格
- `currency`: 货币类型（CNY/USD等）
- `quantity`: 数量
- `discount`: 折扣率（0-1）
- `category_path`: 分类路径
- `brand`: 品牌
- `sku_id`: SKU ID

**推荐行为：** impression, click, view, add_cart, order, payment, purchase

### 3. 新闻场景 (news)

适用于：新闻、文章、资讯等

**核心字段：**
- `read_duration`: 阅读时长（秒）
- `read_progress`: 阅读进度（0-1）
- `word_count`: 文章字数
- `news_type`: 新闻类型（时政/财经/科技等）
- `source`: 新闻来源
- `keywords`: 关键词列表

**推荐行为：** impression, click, read, read_end, like, share, comment

### 4. 音乐场景 (music)

适用于：音乐播放、歌单推荐等

**核心字段：**
- `duration`: 歌曲总时长（秒）
- `play_duration`: 播放时长（秒）
- `artist`: 艺术家
- `album`: 专辑
- `genre`: 音乐风格
- `audio_quality`: 音质（标准/高品/无损）

**推荐行为：** impression, play, play_end, pause, like, add_playlist, share

### 5. 教育场景 (education)

适用于：在线课程、学习资源推荐等

**核心字段：**
- `course_id`: 课程ID
- `lesson_duration`: 课时时长（秒）
- `study_duration`: 学习时长（秒）
- `progress`: 学习进度（0-1）
- `score`: 得分
- `difficulty_level`: 难度等级

**推荐行为：** impression, click, learn, complete, pause, trial, purchase

### 6. 内容推荐场景 (content)

适用于：图文内容、帖子、问答、UGC内容等

**核心字段：**
- `content_type`: 内容类型（post/question/answer/gallery/thread）
- `word_count`: 文字数量
- `image_count`: 图片数量
- `video_count`: 视频数量
- `topic`: 话题/主题
- `content_quality_score`: 内容质量分（0-100）
- `is_original`: 是否原创内容
- `author_level`: 作者等级

**推荐行为：** impression, click, view, read, like, comment, share, collect, report

### 7. 社交推荐场景 (social)

适用于：好友推荐、关注推荐、社交关系链等

**核心字段：**
- `relationship_type`: 关系类型（friend/follow/mutual/suggested）
- `mutual_friends_count`: 共同好友数
- `social_distance`: 社交距离（度数）
- `influence_score`: 影响力分数
- `interaction_frequency`: 互动频率
- `common_interests`: 共同兴趣列表
- `is_verified`: 是否认证用户

**推荐行为：** impression, view_profile, follow, add_friend, message, ignore, block

### 8. 个性化推荐场景 (personalized)

适用于：综合多因素的个性化内容推荐

**核心字段：**
- `personalization_score`: 个性化分数（0-1）
- `match_reasons`: 推荐理由列表
- `user_interests_match`: 用户兴趣匹配度
- `trending_score`: 热度分数
- `novelty_score`: 新颖度分数
- `diversity_group`: 多样性分组
- `recommendation_strategy`: 推荐策略（collaborative/content_based/hybrid）

**推荐行为：** impression, click, positive_feedback, negative_feedback, dismiss, not_interested

---

## 接入方式选择

### 三种接入方式对比

| 接入方式 | 适用场景 | 延迟(P99) | 吞吐量 | 复杂度 | 数据验证 |
|---------|---------|-----------|--------|--------|----------|
| **HTTP API** | C端应用、跨语言集成 | ~100ms | ~5K TPS | 低 | ✅ 自动验证 |
| **gRPC** | 内部服务、性能要求高 | ~50ms | ~10K TPS | 中 | ✅ 类型安全 |
| **Kafka直接写入** | 内部服务、超高频事件 | ~15ms | ~50K TPS | 高 | ⚠️ 客户端验证 |

### ✅ 适合HTTP/gRPC的场景

- **C端应用埋点**：移动App、Web前端
- **需要实时验证**：立即获得数据格式反馈
- **低频事件**：点击、转化、购买等（QPS < 1K）
- **跨语言集成**：Java、Go、Python等多语言客户端

### ✅ 适合Kafka直接写入的场景

- **内部服务埋点**：推荐服务、内容服务等后端服务
- **高频事件**：每秒数千次的曝光、滚动等
- **批量上报**：离线日志补录、数据迁移
- **超低延迟要求**：P99 < 20ms

---

## HTTP/gRPC接入方式

### 快速开始

#### Step 1: 选择或创建场景配置

##### 方式1：使用内置模板（推荐✅）

```bash
# 查看可用模板
curl http://recommender-api:8080/api/v1/tracking-configs/templates/

# 从模板创建配置（以视频场景为例）
curl -X POST "http://recommender-api:8080/api/v1/tracking-configs/from-template" \
  -H "Content-Type: application/json" \
  -d '{
    "tenant_id": "mymx",
    "scenario_id": "vlog_feed",
    "template_id": "video",
    "name": "短视频Feed流埋点配置"
  }'
```

##### 方式2：自定义配置

```bash
curl -X POST "http://recommender-api:8080/api/v1/tracking-configs/" \
  -H "Content-Type: application/json" \
  -d '{
    "tenant_id": "mymx",
    "scenario_id": "vlog_feed",
    "scenario_type": "video",
    "name": "短视频埋点配置",
    "required_fields": [
      {
        "field_name": "duration",
        "field_type": "integer",
        "required": true,
        "description": "视频总时长（秒）",
        "min_value": 1,
        "max_value": 3600
      }
    ]
  }'
```

#### Step 2: 上报埋点数据

##### HTTP方式

```python
import requests

# 视频场景埋点示例
event_data = {
    "tenant_id": "mymx",
    "scenario_id": "vlog_feed",
    "user_id": "user_12345",
    "item_id": "video_67890",
    "action_type": "play_end",
    "context": {
        "device_type": "mobile",
        "os": "iOS",
        "location": "Beijing"
    },
    # 场景特定数据
    "scenario_data": {
        "video_data": {
            "duration": 180,
            "watch_duration": 175,
            "completion_rate": 0.97,
            "video_quality": "1080p",
            "is_fullscreen": true,
            "playback_speed": 1.0
        }
    },
    "position": 3,
    "experiment_id": "exp_001"
}

response = requests.post(
    "http://recommender-api:8080/api/v1/behaviors/track",
    json=event_data
)

print(response.json())
# 输出: {"success": true, "event_id": "evt_abc123", "message": "Event tracked successfully"}
```

##### gRPC方式

```python
import grpc
from recommender.v1 import behavior_pb2, behavior_pb2_grpc

# 创建gRPC channel
channel = grpc.insecure_channel('localhost:50051')
stub = behavior_pb2_grpc.BehaviorServiceStub(channel)

# 构建场景数据
video_data = behavior_pb2.VideoScenarioData(
    duration=180,
    watch_duration=175,
    completion_rate=0.97,
    video_quality="1080p",
    is_fullscreen=True,
    playback_speed=1.0
)

scenario_data = behavior_pb2.ScenarioSpecificData(
    video_data=video_data
)

# 构建事件
event = behavior_pb2.BehaviorEvent(
    tenant_id="mymx",
    scenario_id="vlog_feed",
    user_id="user_12345",
    item_id="video_67890",
    action_type=behavior_pb2.ACTION_TYPE_PLAY_END,
    context=behavior_pb2.BehaviorContext(
        device_type=behavior_pb2.DEVICE_TYPE_MOBILE,
        os="iOS",
        location="Beijing"
    ),
    scenario_data=scenario_data,
    position=3,
    experiment_id="exp_001"
)

# 发送请求
request = behavior_pb2.TrackEventRequest(event=event)
response = stub.TrackEvent(request)

print(f"Success: {response.success}, Event ID: {response.event_id}")
```

### 场景数据格式

#### 视频场景

```json
{
  "scenario_data": {
    "video_data": {
      "duration": 180,
      "watch_duration": 175,
      "completion_rate": 0.97,
      "video_quality": "1080p",
      "is_fullscreen": true,
      "is_muted": false,
      "playback_speed": 1.0,
      "seek_positions": [30, 60, 90],
      "buffer_time": 1200
    }
  }
}
```

#### 电商场景

```json
{
  "scenario_data": {
    "ecommerce_data": {
      "price": 299.90,
      "currency": "CNY",
      "quantity": 2,
      "discount": 0.15,
      "category_path": "/服饰/女装/连衣裙",
      "brand": "某品牌",
      "sku_id": "SKU123456",
      "product_tags": ["夏季", "新款", "热销"],
      "stock_count": 999,
      "coupon_id": "COUPON2024"
    }
  }
}
```

#### 新闻场景

```json
{
  "scenario_data": {
    "news_data": {
      "read_duration": 120,
      "read_progress": 0.85,
      "word_count": 1500,
      "news_type": "科技",
      "source": "某新闻网",
      "author": "张三",
      "keywords": ["人工智能", "技术", "创新"],
      "is_breaking_news": false,
      "publish_time": 1730304000000
    }
  }
}
```

#### 音乐场景

```json
{
  "scenario_data": {
    "music_data": {
      "duration": 240,
      "play_duration": 240,
      "artist": "某歌手",
      "album": "某专辑",
      "genre": "流行",
      "bpm": 120,
      "language": "中文",
      "is_vip_only": false,
      "audio_quality": "无损"
    }
  }
}
```

#### 教育场景

```json
{
  "scenario_data": {
    "education_data": {
      "course_id": "COURSE001",
      "chapter_id": "CHAPTER01",
      "lesson_duration": 1800,
      "study_duration": 1650,
      "progress": 0.92,
      "exercise_count": 10,
      "correct_count": 8,
      "score": 80.0,
      "difficulty_level": "中级",
      "is_certificate_course": true
    }
  }
}
```

#### 内容推荐场景

```json
{
  "scenario_data": {
    "content_data": {
      "content_type": "post",
      "word_count": 800,
      "image_count": 3,
      "video_count": 0,
      "topic": "科技数码",
      "content_quality_score": 85,
      "is_original": true,
      "author_level": "资深作者",
      "has_hashtags": true,
      "hashtags": ["#AI", "#技术分享", "#深度学习"],
      "engagement_rate": 0.08,
      "read_time_estimate": 180
    }
  }
}
```

#### 社交推荐场景

```json
{
  "scenario_data": {
    "social_data": {
      "relationship_type": "suggested",
      "mutual_friends_count": 15,
      "social_distance": 2,
      "influence_score": 78.5,
      "interaction_frequency": "medium",
      "common_interests": ["科技", "旅行", "摄影"],
      "is_verified": true,
      "follower_count": 50000,
      "following_count": 300,
      "post_count": 1200,
      "similarity_score": 0.75
    }
  }
}
```

#### 个性化推荐场景

```json
{
  "scenario_data": {
    "personalized_data": {
      "personalization_score": 0.89,
      "match_reasons": ["兴趣匹配", "热门内容", "好友喜欢"],
      "user_interests_match": 0.85,
      "trending_score": 0.72,
      "novelty_score": 0.65,
      "diversity_group": "tech_news",
      "recommendation_strategy": "hybrid",
      "model_version": "v2.1",
      "feature_importance": {
        "collaborative": 0.4,
        "content_based": 0.3,
        "popularity": 0.2,
        "novelty": 0.1
      }
    }
  }
}
```

### 批量上报

```python
# 批量上报（最多100条）
events = [
    {
        "tenant_id": "mymx",
        "scenario_id": "vlog_feed",
        "user_id": "user_001",
        "item_id": "video_001",
        "action_type": "impression",
        "context": {"device_type": "mobile"},
        "position": 1
    },
    {
        "tenant_id": "mymx",
        "scenario_id": "vlog_feed",
        "user_id": "user_001",
        "item_id": "video_002",
        "action_type": "impression",
        "context": {"device_type": "mobile"},
        "position": 2
    }
]

response = requests.post(
    "http://recommender-api:8080/api/v1/behaviors/track/batch",
    json={"events": events}
)

print(response.json())
# 输出: {"success": true, "total": 2, "succeeded": 2, "failed": 0}
```

### Python SDK示例

```python
class TrackingClient:
    """埋点客户端封装"""
    
    def __init__(self, base_url: str, tenant_id: str):
        self.base_url = base_url
        self.tenant_id = tenant_id
    
    def track_video_play(
        self,
        user_id: str,
        video_id: str,
        scenario_id: str,
        duration: int,
        watch_duration: int,
        completion_rate: float,
        **kwargs
    ):
        """视频播放结束埋点"""
        event = {
            "tenant_id": self.tenant_id,
            "scenario_id": scenario_id,
            "user_id": user_id,
            "item_id": video_id,
            "action_type": "play_end",
            "context": kwargs.get("context", {"device_type": "mobile"}),
            "scenario_data": {
                "video_data": {
                    "duration": duration,
                    "watch_duration": watch_duration,
                    "completion_rate": completion_rate,
                    "video_quality": kwargs.get("video_quality", "720p"),
                    "is_fullscreen": kwargs.get("is_fullscreen", False),
                    "playback_speed": kwargs.get("playback_speed", 1.0)
                }
            }
        }
        
        return self._send_event(event)
    
    def _send_event(self, event: dict) -> dict:
        """发送事件"""
        url = f"{self.base_url}/api/v1/behaviors/track"
        response = requests.post(url, json=event, timeout=3)
        response.raise_for_status()
        return response.json()

# 使用示例
client = TrackingClient(
    base_url="http://recommender-api:8080",
    tenant_id="mymx"
)

result = client.track_video_play(
    user_id="user_12345",
    video_id="video_67890",
    scenario_id="vlog_feed",
    duration=180,
    watch_duration=175,
    completion_rate=0.97,
    video_quality="1080p",
    is_fullscreen=True
)

print(f"✅ 埋点成功: {result['event_id']}")
```

---

## Kafka直接接入方式

### 📋 适用场景

#### ✅ 适合直接写Kafka的场景

- **内部服务埋点**：推荐服务、内容服务等后端服务
- **高频事件**：每秒数千次的曝光、滚动等
- **批量上报**：离线日志补录、数据迁移
- **低延迟要求**：实时性要求高的场景

#### ❌ 不适合的场景

- **C端埋点**：前端、移动端（应使用HTTP/SDK）
- **需要实时验证**：需要立即返回验证结果
- **低频事件**：每秒几次的点击、转化（HTTP即可满足）

### 🚀 快速开始

#### 1. Kafka连接信息

```yaml
# 开发环境
bootstrap_servers: localhost:9092
security_protocol: PLAINTEXT

# 生产环境
bootstrap_servers: kafka-1:9092,kafka-2:9092,kafka-3:9092
security_protocol: SASL_SSL
sasl_mechanism: PLAIN
sasl_username: ${KAFKA_USER}
sasl_password: ${KAFKA_PASSWORD}
```

#### 2. Topic命名规范

**重要：按租户隔离！** 🔒

```
Topic格式: user-behaviors-{tenant_id}

示例：
- user-behaviors-mymx           # mymx租户的行为数据
- user-behaviors-tenant_abc     # tenant_abc租户的行为数据
```

**分区配置**：
- 分区数：12-24（根据流量调整）
- 副本数：3（生产环境）
- 保留时间：7天

#### 3. 消息格式（JSON Schema）

```json
{
  "event_id": "evt_abc123",                    // 必填：事件唯一ID
  "tenant_id": "mymx",                         // 必填：租户ID（SaaS隔离）
  "scenario_id": "vlog_feed",                  // 必填：场景ID
  "user_id": "user_12345",                     // 必填：用户ID
  "item_id": "item_67890",                     // 必填：物品ID
  "action_type": "click",                      // 必填：行为类型（见枚举）
  "timestamp": 1730304000000,                  // 必填：毫秒时间戳
  
  "context": {                                 // 必填：上下文信息
    "device_type": "mobile",                   // 必填：mobile/pc/tablet
    "os": "iOS",                               // 可选
    "location": "Beijing",                     // 可选
    "ip": "192.168.1.1"                        // 可选
  },
  
  "experiment_id": "exp_001",                  // 可选：AB实验ID
  "experiment_group": "treatment",             // 可选：实验分组
  "position": 3,                               // 可选：推荐位置
  
  "scenario_data": {                           // 可选：场景特定数据
    "video_data": {                            // 视频场景
      "duration": 180,
      "watch_duration": 175,
      "completion_rate": 0.97,
      "video_quality": "1080p"
    }
  },
  
  "extra_data": {}                             // 可选：额外数据（JSON）
}
```

### 💻 代码示例

#### Python (aiokafka - 推荐)

```python
from aiokafka import AIOKafkaProducer
import json
import asyncio
from datetime import datetime
import uuid

class KafkaTracker:
    """Kafka埋点客户端"""
    
    def __init__(self, bootstrap_servers: str, tenant_id: str):
        self.bootstrap_servers = bootstrap_servers
        self.tenant_id = tenant_id
        self.producer = None
        self.topic = f"user-behaviors-{tenant_id}"
    
    async def start(self):
        """启动生产者"""
        self.producer = AIOKafkaProducer(
            bootstrap_servers=self.bootstrap_servers,
            value_serializer=lambda v: json.dumps(v).encode('utf-8'),
            compression_type='gzip',  # 启用压缩
            acks='all',               # 确保写入成功
            retries=3,                # 重试3次
            max_in_flight_requests_per_connection=5
        )
        await self.producer.start()
        print(f"✅ Kafka Producer started: {self.topic}")
    
    async def track(
        self,
        scenario_id: str,
        user_id: str,
        item_id: str,
        action_type: str,
        device_type: str = "mobile",
        **kwargs
    ):
        """上报单个事件"""
        event = {
            "event_id": f"evt_{uuid.uuid4().hex[:16]}",
            "tenant_id": self.tenant_id,
            "scenario_id": scenario_id,
            "user_id": user_id,
            "item_id": item_id,
            "action_type": action_type,
            "timestamp": int(datetime.now().timestamp() * 1000),
            "context": {
                "device_type": device_type,
                **kwargs.get("context", {})
            }
        }
        
        # 添加可选字段
        if "position" in kwargs:
            event["position"] = kwargs["position"]
        if "scenario_data" in kwargs:
            event["scenario_data"] = kwargs["scenario_data"]
        if "experiment_id" in kwargs:
            event["experiment_id"] = kwargs["experiment_id"]
        
        # 发送到Kafka（Key为user_id，保证同一用户有序）
        await self.producer.send(
            self.topic,
            value=event,
            key=user_id.encode('utf-8')
        )
    
    async def track_batch(self, events: list):
        """批量上报"""
        tasks = []
        for event in events:
            # 补充必填字段
            if "event_id" not in event:
                event["event_id"] = f"evt_{uuid.uuid4().hex[:16]}"
            if "timestamp" not in event:
                event["timestamp"] = int(datetime.now().timestamp() * 1000)
            if "tenant_id" not in event:
                event["tenant_id"] = self.tenant_id
            
            tasks.append(
                self.producer.send(
                    self.topic,
                    value=event,
                    key=event["user_id"].encode('utf-8')
                )
            )
        
        # 批量发送
        await asyncio.gather(*tasks)
    
    async def close(self):
        """关闭生产者"""
        if self.producer:
            await self.producer.stop()
            print("✅ Kafka Producer closed")


# 使用示例
async def main():
    tracker = KafkaTracker(
        bootstrap_servers="localhost:9092",
        tenant_id="mymx"
    )
    
    await tracker.start()
    
    # 单个事件
    await tracker.track(
        scenario_id="vlog_feed",
        user_id="user_123",
        item_id="video_456",
        action_type="click",
        device_type="mobile",
        position=3,
        scenario_data={
            "video_data": {
                "duration": 180,
                "watch_duration": 175,
                "completion_rate": 0.97
            }
        }
    )
    
    # 批量事件（曝光）
    impressions = [
        {
            "scenario_id": "vlog_feed",
            "user_id": "user_123",
            "item_id": f"video_{i}",
            "action_type": "impression",
            "context": {"device_type": "mobile"},
            "position": i
        }
        for i in range(1, 11)  # 10个曝光
    ]
    await tracker.track_batch(impressions)
    
    await tracker.close()

if __name__ == "__main__":
    asyncio.run(main())
```

#### Go (sarama)

```go
package main

import (
    "encoding/json"
    "fmt"
    "time"
    "github.com/Shopify/sarama"
    "github.com/google/uuid"
)

type KafkaTracker struct {
    producer  sarama.SyncProducer
    tenantID  string
    topic     string
}

func NewKafkaTracker(brokers []string, tenantID string) (*KafkaTracker, error) {
    config := sarama.NewConfig()
    config.Producer.Return.Successes = true
    config.Producer.Compression = sarama.CompressionGZIP
    config.Producer.RequiredAcks = sarama.WaitForAll
    config.Producer.Retry.Max = 3
    
    producer, err := sarama.NewSyncProducer(brokers, config)
    if err != nil {
        return nil, err
    }
    
    return &KafkaTracker{
        producer: producer,
        tenantID: tenantID,
        topic:    fmt.Sprintf("user-behaviors-%s", tenantID),
    }, nil
}

func (t *KafkaTracker) Track(event map[string]interface{}) error {
    // 补充必填字段
    if _, ok := event["event_id"]; !ok {
        event["event_id"] = fmt.Sprintf("evt_%s", uuid.New().String()[:16])
    }
    if _, ok := event["timestamp"]; !ok {
        event["timestamp"] = time.Now().UnixNano() / 1e6
    }
    if _, ok := event["tenant_id"]; !ok {
        event["tenant_id"] = t.tenantID
    }
    
    // 序列化
    msgBytes, err := json.Marshal(event)
    if err != nil {
        return err
    }
    
    // 发送到Kafka
    userID := event["user_id"].(string)
    msg := &sarama.ProducerMessage{
        Topic: t.topic,
        Key:   sarama.StringEncoder(userID),
        Value: sarama.ByteEncoder(msgBytes),
    }
    
    _, _, err = t.producer.SendMessage(msg)
    return err
}

func (t *KafkaTracker) Close() error {
    return t.producer.Close()
}

// 使用示例
func main() {
    tracker, err := NewKafkaTracker(
        []string{"localhost:9092"},
        "mymx",
    )
    if err != nil {
        panic(err)
    }
    defer tracker.Close()
    
    // 上报点击事件
    event := map[string]interface{}{
        "scenario_id": "vlog_feed",
        "user_id":     "user_123",
        "item_id":     "video_456",
        "action_type": "click",
        "context": map[string]interface{}{
            "device_type": "mobile",
        },
        "position": 3,
    }
    
    if err := tracker.Track(event); err != nil {
        fmt.Printf("Failed to track: %v\n", err)
    } else {
        fmt.Println("✅ Event tracked successfully")
    }
}
```

#### Java (Kafka Clients)

```java
import org.apache.kafka.clients.producer.*;
import com.fasterxml.jackson.databind.ObjectMapper;
import java.util.*;

public class KafkaTracker {
    private final KafkaProducer<String, String> producer;
    private final String tenantId;
    private final String topic;
    private final ObjectMapper mapper;
    
    public KafkaTracker(String bootstrapServers, String tenantId) {
        Properties props = new Properties();
        props.put("bootstrap.servers", bootstrapServers);
        props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
        props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");
        props.put("compression.type", "gzip");
        props.put("acks", "all");
        props.put("retries", 3);
        
        this.producer = new KafkaProducer<>(props);
        this.tenantId = tenantId;
        this.topic = "user-behaviors-" + tenantId;
        this.mapper = new ObjectMapper();
    }
    
    public void track(Map<String, Object> event) throws Exception {
        // 补充必填字段
        event.putIfAbsent("event_id", "evt_" + UUID.randomUUID().toString().substring(0, 16));
        event.putIfAbsent("timestamp", System.currentTimeMillis());
        event.putIfAbsent("tenant_id", tenantId);
        
        // 序列化
        String json = mapper.writeValueAsString(event);
        String userId = (String) event.get("user_id");
        
        // 发送
        ProducerRecord<String, String> record = new ProducerRecord<>(
            topic,
            userId,  // Key
            json     // Value
        );
        
        producer.send(record, (metadata, exception) -> {
            if (exception != null) {
                System.err.println("Failed to send: " + exception.getMessage());
            } else {
                System.out.println("✅ Sent to partition " + metadata.partition());
            }
        });
    }
    
    public void close() {
        producer.close();
    }
}
```

### 🔒 安全注意事项

#### 1. 租户隔离（必须！）

```python
# ✅ 正确：使用租户专属Topic
topic = f"user-behaviors-{tenant_id}"

# ❌ 错误：所有租户共用Topic
topic = "user-behaviors"  # 不要这样做！
```

#### 2. 数据验证

虽然绕过了behavior-service，但客户端仍需验证：

```python
def validate_event(event: dict) -> bool:
    """客户端验证"""
    required_fields = [
        'tenant_id', 'scenario_id', 'user_id',
        'item_id', 'action_type', 'context'
    ]
    
    # 检查必填字段
    for field in required_fields:
        if field not in event:
            raise ValueError(f"Missing required field: {field}")
    
    # 检查device_type
    if 'device_type' not in event.get('context', {}):
        raise ValueError("context.device_type is required")
    
    return True
```

#### 3. 错误处理

```python
# 重试机制
async def track_with_retry(tracker, event, max_retries=3):
    for attempt in range(max_retries):
        try:
            await tracker.track(**event)
            return True
        except Exception as e:
            if attempt == max_retries - 1:
                logger.error(f"Failed after {max_retries} retries: {e}")
                # 可选：写入本地队列，稍后重试
                save_to_local_queue(event)
                return False
            await asyncio.sleep(2 ** attempt)  # 指数退避
```

### 📊 性能对比

#### 延迟对比

| 方式 | P50 | P99 | P999 |
|-----|-----|-----|------|
| HTTP API | 50ms | 100ms | 200ms |
| gRPC | 20ms | 50ms | 100ms |
| **Kafka直接写入** | **5ms** | **15ms** | **30ms** |

#### 吞吐对比

| 方式 | 单机TPS |
|-----|---------|
| HTTP API | ~5K |
| gRPC | ~10K |
| **Kafka直接写入** | **~50K** |

### 🎯 Kafka最佳实践

#### 1. 批量写入

```python
# ❌ 不推荐：循环单条发送
for event in events:
    await tracker.track(**event)

# ✅ 推荐：批量发送
await tracker.track_batch(events)
```

#### 2. 异步非阻塞

```python
# ✅ 推荐：异步发送，不等待确认
async def track_async(event):
    asyncio.create_task(tracker.track(**event))
    # 立即返回，不阻塞业务逻辑
```

#### 3. 本地队列 + 定时刷新

```python
from collections import deque
import asyncio

class BufferedKafkaTracker:
    def __init__(self, tracker, buffer_size=100, flush_interval=1.0):
        self.tracker = tracker
        self.buffer = deque(maxlen=buffer_size)
        self.flush_interval = flush_interval
        self._start_flusher()
    
    async def track(self, **event):
        """添加到缓冲区"""
        self.buffer.append(event)
        
        # 缓冲区满了，立即刷新
        if len(self.buffer) >= self.buffer.maxlen:
            await self._flush()
    
    async def _flush(self):
        """刷新缓冲区到Kafka"""
        if not self.buffer:
            return
        
        events = list(self.buffer)
        self.buffer.clear()
        
        await self.tracker.track_batch(events)
    
    def _start_flusher(self):
        """定时刷新"""
        async def flush_loop():
            while True:
                await asyncio.sleep(self.flush_interval)
                await self._flush()
        
        asyncio.create_task(flush_loop())
```

---

## Proto代码生成指南

### 📁 目录结构

```
/Users/edy/Lemo/andrew-protos/          # Proto定义仓库
├── protos/
│   └── recommender/
│       └── v1/
│           ├── behavior.proto          # 行为采集服务（已更新✅）
│           ├── item.proto
│           ├── scenario.proto
│           └── ...
├── Makefile                            # 构建脚本
└── gen/                                # 生成的代码
    ├── go/                             # Go代码
    └── python/                         # Python代码

/Users/edy/PycharmProjects/lemo_recommender/  # 推荐系统服务
└── app/
    └── grpc_generated/
        └── python/                     # 复制生成的Python代码到这里
```

### 🚀 生成步骤

#### Step 1: 更新Proto文件

Proto文件已更新，支持场景扩展字段：
- ✅ `VideoScenarioData` - 视频场景数据
- ✅ `EcommerceScenarioData` - 电商场景数据
- ✅ `NewsScenarioData` - 新闻场景数据
- ✅ `MusicScenarioData` - 音乐场景数据
- ✅ `EducationScenarioData` - 教育场景数据
- ✅ `ScenarioSpecificData` - 场景数据联合类型

位置: `/Users/edy/Lemo/andrew-protos/protos/recommender/v1/behavior.proto`

#### Step 2: 生成代码

```bash
# 进入andrew-protos目录
cd /Users/edy/Lemo/andrew-protos

# 方式1：生成所有语言代码（推荐）
make generate

# 方式2：仅生成Python代码
make generate-python

# 方式3：使用protoc直接生成
make generate-protoc
```

生成的Python代码位置：
```
/Users/edy/Lemo/andrew-protos/gen/python/recommender/v1/
├── behavior_pb2.py          # 消息定义
├── behavior_pb2_grpc.py     # 服务定义
└── behavior_pb2.pyi         # 类型提示
```

#### Step 3: 复制到推荐系统项目

```bash
# 复制生成的Python代码
cp -r /Users/edy/Lemo/andrew-protos/gen/python/* \
      /Users/edy/PycharmProjects/lemo_recommender/app/grpc_generated/python/

# 或者使用脚本（如果有）
cd /Users/edy/PycharmProjects/lemo_recommender
./scripts/update_grpc_code.sh
```

#### Step 4: 验证生成结果

```bash
# 检查文件是否存在
ls -la /Users/edy/PycharmProjects/lemo_recommender/app/grpc_generated/python/recommender/v1/

# 应该看到：
# - behavior_pb2.py
# - behavior_pb2_grpc.py
# - behavior_pb2.pyi
```

#### Step 5: 更新gRPC服务实现

生成新代码后，需要更新gRPC服务实现以支持新的场景数据类型。

文件位置: `/Users/edy/PycharmProjects/lemo_recommender/app/grpc_server/services/behavior_service.py`

主要更新点：
1. `_event_proto_to_dict` 方法需要处理 `ScenarioSpecificData`
2. 支持解析不同场景的数据（video_data, ecommerce_data等）

### 🔧 使用生成的代码

#### 1. 在Python中导入

```python
# 导入生成的protobuf代码
import sys
from pathlib import Path

# 添加grpc_generated到路径
grpc_gen_path = Path(__file__).parent.parent.parent / "grpc_generated" / "python"
sys.path.insert(0, str(grpc_gen_path))

# 导入消息和服务
from recommender.v1 import behavior_pb2, behavior_pb2_grpc
```

#### 2. 创建场景数据

```python
# 视频场景数据
video_data = behavior_pb2.VideoScenarioData(
    duration=180,
    watch_duration=175,
    completion_rate=0.97,
    video_quality="1080p",
    is_fullscreen=True,
    playback_speed=1.0,
    seek_positions=[30, 60, 90],
    buffer_time=1200
)

# 封装为ScenarioSpecificData
scenario_data = behavior_pb2.ScenarioSpecificData(
    video_data=video_data
)

# 创建事件
event = behavior_pb2.BehaviorEvent(
    tenant_id="mymx",
    scenario_id="vlog_feed",
    user_id="user_12345",
    item_id="video_67890",
    action_type=behavior_pb2.ACTION_TYPE_PLAY_END,
    context=behavior_pb2.BehaviorContext(
        device_type=behavior_pb2.DEVICE_TYPE_MOBILE
    ),
    scenario_data=scenario_data
)
```

#### 3. 处理oneof字段

```python
# 检查使用了哪个场景数据
which = scenario_data.WhichOneof('data')

if which == 'video_data':
    video_data = scenario_data.video_data
    print(f"Duration: {video_data.duration}")
elif which == 'ecommerce_data':
    ecommerce_data = scenario_data.ecommerce_data
    print(f"Price: {ecommerce_data.price}")
```

### 🔄 完整工作流

```bash
# 1. 修改Proto文件
vim /Users/edy/Lemo/andrew-protos/protos/recommender/v1/behavior.proto

# 2. 生成代码
cd /Users/edy/Lemo/andrew-protos
make generate

# 3. 复制到项目
cp -r gen/python/* /Users/edy/PycharmProjects/lemo_recommender/app/grpc_generated/python/

# 4. 更新服务实现
vim /Users/edy/PycharmProjects/lemo_recommender/app/grpc_server/services/behavior_service.py

# 5. 测试
cd /Users/edy/PycharmProjects/lemo_recommender
poetry run pytest tests/test_behavior_service.py

# 6. 重启服务
docker restart lemo_recommender
# 或
kubectl rollout restart deployment/recommender-service
```

---

## 配置管理

### 查询配置

```bash
# 获取特定场景的配置
curl "http://recommender-api:8080/api/v1/tracking-configs/mymx/vlog_feed"

# 列出所有配置（支持筛选）
curl "http://recommender-api:8080/api/v1/tracking-configs/?tenant_id=mymx&scenario_type=video"
```

### 更新配置

```bash
curl -X PUT "http://recommender-api:8080/api/v1/tracking-configs/mymx/vlog_feed" \
  -H "Content-Type: application/json" \
  -d '{
    "description": "更新后的描述",
    "optional_fields": [
      {
        "field_name": "buffer_time",
        "field_type": "integer",
        "required": false,
        "description": "缓冲时间（毫秒）"
      }
    ]
  }'
```

### 删除配置

```bash
# 软删除（设置为不活跃）
curl -X DELETE "http://recommender-api:8080/api/v1/tracking-configs/mymx/vlog_feed"
```

---

## 验证和调试

### 1. 验证模式（不实际发送）

```python
# 仅验证数据格式，不发送到Kafka
response = requests.post(
    "http://recommender-api:8080/api/v1/behaviors/validate",
    json=event_data
)

if response.json()["success"]:
    print("✅ 数据格式正确")
else:
    print("❌ 数据格式错误:", response.json())
```

### 2. 查看验证警告

如果场景数据不符合配置，会返回警告：

```python
response = requests.post(
    "http://recommender-api:8080/api/v1/behaviors/track",
    json=event_data
)

result = response.json()
if "validation_warnings" in result:
    print("⚠️  验证警告:", result["validation_warnings"])
```

### 3. 健康检查

```bash
curl "http://recommender-api:8080/api/v1/behaviors/health"
```

输出示例：
```json
{
  "status": "healthy",
  "kafka_available": true,
  "stats": {
    "total_events": 10000,
    "kafka_success": 9995,
    "kafka_failed": 5,
    "rejected": 0,
    "validation_warnings": 10,
    "success_rate": 99.95
  }
}
```

---

## 最佳实践

### 1. 字段设计原则

✅ **必填字段尽量少** - 降低接入成本  
✅ **字段命名规范** - 使用小写+下划线（snake_case）  
✅ **设置合理范围** - 使用min_value/max_value限制  
✅ **使用枚举值** - 限制字符串字段的取值范围

### 2. 数据采集建议

- **曝光事件（impression）**: 物品进入视口时上报
- **点击事件（click）**: 用户点击时立即上报
- **播放/阅读结束**: 用户离开或完成时上报（包含时长、进度等）
- **批量上报**: 曝光事件建议批量上报（每秒或每10条）

### 3. 性能优化

```python
# ❌ 不推荐：每个曝光单独上报
for item in items:
    track_event(item, action="impression")

# ✅ 推荐：批量上报曝光
track_batch([
    {"item_id": item["id"], "action_type": "impression"}
    for item in items
])
```

### 4. 错误处理

```python
try:
    response = requests.post(url, json=event, timeout=3)
    response.raise_for_status()
except requests.exceptions.Timeout:
    # 超时：记录日志，稍后重试
    logger.warning("Tracking timeout, will retry")
    retry_queue.append(event)
except requests.exceptions.RequestException as e:
    # 其他错误：记录日志
    logger.error(f"Tracking failed: {e}")
```

### 5. 监控指标

关键指标：
- 上报成功率 > 99.9%
- 接口响应时间 < 100ms（P99）
- Kafka发送成功率 > 99.9%
- 数据验证通过率 > 95%

### 6. 环境隔离

```python
# 开发/生产环境使用不同配置
KAFKA_CONFIG = {
    "dev": {
        "bootstrap_servers": "localhost:9092",
        "security_protocol": "PLAINTEXT"
    },
    "prod": {
        "bootstrap_servers": "kafka-1:9092,kafka-2:9092,kafka-3:9092",
        "security_protocol": "SASL_SSL",
        "sasl_username": os.getenv("KAFKA_USER"),
        "sasl_password": os.getenv("KAFKA_PASSWORD")
    }
}
```

---

## 常见问题

### Q1: 如何扩展自定义场景？

A: 有两种方式：

**方式1：使用custom_data（灵活，无验证）**
```json
{
  "scenario_data": {
    "custom_data": {
      "custom_field_1": "value1",
      "custom_field_2": 123
    }
  }
}
```

**方式2：创建自定义配置（推荐，有验证）**
```bash
curl -X POST "/api/v1/tracking-configs/" \
  -d '{
    "scenario_type": "custom",
    "required_fields": [...]
  }'
```

### Q2: 场景配置更新后，旧数据怎么办？

A: 
- 旧数据仍然有效（向后兼容）
- 新字段会有默认值或标记为缺失
- 建议使用版本号管理配置变更

### Q3: 必填字段缺失会怎样？

A: 
- 基础必填字段（tenant_id等）缺失会被拒绝（返回400）
- 场景特定字段缺失只会产生警告（不影响上报）

### Q4: 如何测试埋点数据？

A:
```python
# 1. 使用验证接口
response = requests.post("/api/v1/behaviors/validate", json=event)

# 2. 查看Kafka消息
kafka-console-consumer --topic user-behaviors-mymx

# 3. 查询ClickHouse（Flink处理后）
SELECT * FROM user_behaviors WHERE scenario_id = 'vlog_feed' LIMIT 10;
```

### Q5: HTTP vs gRPC vs Kafka，如何选择？

A:
- **C端应用** → HTTP API（简单、兼容性好）
- **内部服务（低频）** → gRPC（类型安全、性能好）
- **内部服务（高频）** → Kafka直接写入（超高性能）

### Q6: Kafka直接写入会绕过验证吗？

A: 是的，但建议：
- 客户端实现数据验证逻辑
- 定期监控数据质量
- 使用 Flink 进行数据质量监控

### Q7: Proto文件更新后需要重启服务吗？

A: 是的，需要：
1. 生成新的Python代码
2. 复制到项目
3. 重启推荐系统服务

```bash
# 重启Docker容器
docker restart lemo_recommender

# 或K8s Pod
kubectl rollout restart deployment/recommender-service
```

---

## 📞 技术支持

- **文档**: [系统设计.md](./系统设计.md)
- **API文档**: http://recommender-api:8080/api/v1/docs
- **gRPC定义**: /Users/edy/Lemo/andrew-protos/protos/recommender/v1/behavior.proto
- **架构方案**: [数据分析架构方案.md](./数据分析架构方案.md)

---

**文档版本**: v1.0  
**最后更新**: 2025-10-30  
**维护者**: 推荐系统团队

